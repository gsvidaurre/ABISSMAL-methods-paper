---
title: "Interobserver reliability"
author: "Grace Smith-Vidaurre"
date: "2024-07-18"
output: html_document
---

Purpose: Calculate interobserver reliability for a small set of videos scored by two observers prior to scoring the full set of videos that will be used for the first round of ABISSMAL validation. See the script "validation_approach.Rmd" for more information about how videos were selected for validation. Videos were scored in BORIS using protocol written for video scoring, and an ethogram with four behaviors (exit, entrance, perching, and inside of the container) for coarse-grained validation. Each observer scored the same masked set of 25 unique recording events (50 videos total). 

```{r libraries and paths, warning = FALSE, message = FALSE}

rm(list = ls())

library(tidyverse)
library(pbapply)
library(data.table)
library(irr)
library(lubridate)

path <- "~/Desktop/ABISSMAL_BORIS_InterObserverReliability"

```

Read in all of the spreadsheets across both observers. Each spreadsheet represents events scored for one of 25 unique recording events, or 25 pairs of videos recorded by motion-detection.
```{r}

# Print all of the directories with video files
files <- list.files(path, pattern = ".csv$", full.names = TRUE)
length(files)

combined_events <- data.table::rbindlist(pblapply(1:length(files), function(i){
  
  read.csv(files[i])

}))

glimpse(combined_events)

# There should be 25 unique recording events or pairs of media file names
# The 50 masked videos have numeric ids for the unique recording event and then the video within each pair
n <- 25
video_nms <- paste(paste(rep(paste("video", seq(1, n, 1), sep = "_"), each = 2), seq(1, 2, 1), sep = "_"), ".mp4", sep = "")
video_nms

```

Find the unique observers in the dataset.
```{r}

# Remove the unique video file names to return the unique paths per observer
user_paths <- unique(gsub(paste(video_nms, collapse = "|"), "", combined_events$Media.file.name))
user_paths

# Add a new column specifying unique numeric ids per observer to the data frame
combined_events <- combined_events %>% 
  dplyr::mutate(
    user_paths = ifelse(grepl(user_paths[1], Media.file.name), user_paths[1], user_paths[2]),
    user_id = ifelse(grepl(user_paths[1], Media.file.name), 1, 2)
  )

glimpse(combined_events)

# Checking, looks good
combined_events %>% 
  distinct(
    user_paths, user_id
  )

```

# Interobserver reliability for categorical scoring

For each unique recording event, arrange the events scored by each observer. Focus on the behavior (the ethogram categories) and the type (start or stop) for now.
```{r}

# I need unique recording events, not unique video names here
rec_nms <- paste(paste("video", seq(1, n, 1), sep = "_"), "_", sep = "")
rec_nms

# Unique subjects in the results
unique(combined_events$Subject)

interobs_res <- data.table::rbindlist(pblapply(1:length(rec_nms), function(i){

  behav_sum <- combined_events %>% 
    dplyr::filter(
      grepl(rec_nms[i], Media.file.name)
    ) %>%
    # Standardize subject names
    dplyr::mutate(
      Subject = ifelse(Subject == "Adult female", "Female", Subject),
      Subject = ifelse(Subject == "Adult male", "Male", Subject),
      Subject = ifelse(Subject == "Juveniles_pooled", "Juveniles_Pooled", Subject)
    ) %>% 
    # Arrange the events by timestamps, and then by subject in order to avoid arbitrary ordering of events when the timestamps are the same across subjects and observers
    dplyr::arrange(
      -desc(Time)
    ) %>%
    dplyr::arrange(
      -desc(Subject)
    ) %>% 
    dplyr::select(user_id, Subject, Behavior, Behavior.type) %>%
    # Make a column of the behavior plus the type (start or stop)
    dplyr::mutate(
      Score = paste(Subject, Behavior, Behavior.type, sep = "-")
    ) %>%
    dplyr::select(-c(Subject, Behavior, Behavior.type)) %>%
    group_by(user_id) %>%
    # Make a unique row id for pivoting within groups (users)
    dplyr::mutate(
      row = row_number()
    ) %>% 
    pivot_wider(
      names_from = "user_id",
      values_from = c("Score")
    )
  
  # Calculate (unweighted) Cohen's kappa as interobserver reliability between two raters. A higher value of kappa indicates greater agreement, and a value of 1 indicates perfect agreement between raters. A statistically significant p-value at an alpha of 0.05 means that the observers agree more than expected by chance (null hypothesis of chance agreement is rejected)
  kappa_res <- kappa2(
    behav_sum %>% 
      dplyr::select(`1`, `2`),
    weight = "unweighted",
    sort.levels = FALSE
  )
  
  return(
    data.frame(
      recording_event = i,
      kappa = round(kappa_res$value, 4),
      p_val = round(kappa_res$p.value, 4)
    )
  )

}))

# glimpse(interobs_res)

interobs_res %>% 
  print(nrows = nrow(.))

```

Summarize the interobserver reliability results.
```{r}

# Mean and interquartile range of kappa
interobs_res %>% 
  dplyr::summarise(
    mean_kappa = mean(kappa),
    IQR_kappa = IQR(kappa)
  )

  # mean_kappa IQR_kappa
# 1   0.609908    0.5714

# Perfect agreement for 12 unique recording events, or 48% of the 25 recording events
interobs_res %>% 
  dplyr::filter(kappa == 1) %>% 
  nrow(.)

# Statistically significant agreement for 17 of the 25 recording events, or 68% of the recording events
interobs_res %>% 
  dplyr::filter(p_val <= 0.05) %>% 
  nrow(.)

# There were 6 videos for which we disagreed
interobs_res %>% 
  dplyr::filter(kappa < 1 & (p_val >= 0.05 | is.na(p_val))) %>% 
  nrow(.)

# See these kappa values
interobs_res %>% 
  dplyr::filter(kappa < 1 & (p_val >= 0.05 | is.na(p_val)))

# Get the unique recording event IDs for the videos with disagreement
disag <- interobs_res %>% 
  dplyr::filter(kappa < 1 & (p_val >= 0.05 | is.na(p_val))) %>% 
  pull(recording_event)

```

Check out the scoring results for the videos with disagreement.
```{r}

# I need unique recording events, not unique video names here
disag_nms <- paste(paste("video", disag, sep = "_"), "_", sep = "")
disag_nms

disag_res <- data.table::rbindlist(pblapply(1:length(disag_nms), function(i){

  behav_sum <- combined_events %>% 
    dplyr::filter(
      grepl(disag_nms[i], Media.file.name)
    ) %>%
    # Standardize subject names
    dplyr::mutate(
      Subject = ifelse(Subject == "Adult female", "Female", Subject),
      Subject = ifelse(Subject == "Adult male", "Male", Subject),
      Subject = ifelse(Subject == "Juveniles_pooled", "Juveniles_Pooled", Subject)
    ) %>% 
    # Arrange the events by timestamps, and then by subject in order to avoid arbitrary ordering of events when the timestamps are the same across subjects and observers
    dplyr::arrange(
      -desc(Time)
    ) %>%
    dplyr::arrange(
      -desc(Subject)
    ) %>% 
    dplyr::select(user_id, Subject, Behavior, Behavior.type) %>%
    # Make a column of the behavior plus the type (start or stop)
    dplyr::mutate(
      Score = paste(Subject, Behavior, Behavior.type, sep = "-")
    ) %>%
    dplyr::select(-c(Subject, Behavior, Behavior.type)) %>%
    group_by(user_id) %>%
    # Make a unique row id for pivoting within groups (users)
    dplyr::mutate(
      row = row_number()
    ) %>% 
    pivot_wider(
      names_from = "user_id",
      values_from = c("Score")
    )

  return(
    behav_sum %>% 
      dplyr::mutate(
        recording_event = disag[i]
      ) %>% 
      dplyr::select(
        recording_event, `1`, `2`
      )
  )

}))

disag_res %>%
  print(n = nrow(.))

```

Summary of results for categorical scoring agreement: Both observers are in pretty high agreement (mean kappa of 0.60 and perfect agreement across 48% of the recording events), but there are two clear sources of variation that we need to standardize between observers. First, I'm separating juveniles pooled versus individual juveniles. We should standardize our scoring such that individual juveniles are scored as individuals, and when there is more than one juvenile, individuals are pooled. Second, for recording event 21, I did not score an entrance event for the male, so we need to doublecheck our respective scoring for this video.

# Comparing timestamps between observers

Next, we need to check the timestamps of events between observers, for the videos for which we had perfect agreement in categorical scores.
```{r}

# Get the unique recording event IDs for the videos with perfect agreement
agree <- interobs_res %>% 
  dplyr::filter(kappa == 1) %>% 
  pull(recording_event)

# I need unique recording events, not unique video names here
agree_nms <- paste(paste("video", agree, sep = "_"), "_", sep = "")
agree_nms

agree_res <- data.table::rbindlist(pblapply(1:length(agree), function(i){

  behav_sum <- combined_events %>% 
    dplyr::filter(
      grepl(agree_nms[i], Media.file.name)
    ) %>%
    # Standardize subject names
    dplyr::mutate(
      Subject = ifelse(Subject == "Adult female", "Female", Subject),
      Subject = ifelse(Subject == "Adult male", "Male", Subject),
      Subject = ifelse(Subject == "Juveniles_pooled", "Juveniles_Pooled", Subject)
    ) %>% 
    # Arrange the events by timestamps, and then by subject in order to avoid arbitrary ordering of events when the timestamps are the same across subjects and observers
    dplyr::arrange(
      -desc(Time)
    ) %>%
    dplyr::arrange(
      -desc(Subject)
    ) %>% 
    dplyr::select(user_id, Subject, Behavior, Behavior.type, Time) %>%
    # Make a column of the behavior plus the type (start or stop)
    dplyr::mutate(
      Score = paste(Subject, Behavior, Behavior.type, sep = "-")
    ) %>%
    dplyr::select(-c(Subject, Behavior, Behavior.type)) %>%
    group_by(user_id) %>%
    # Make a unique row id for pivoting within groups (users)
    dplyr::mutate(
      row = row_number()
    ) %>% 
    pivot_wider(
      names_from = "user_id",
      values_from = c("Score", "Time")
    ) %>% 
    # Calculate the time difference between observers (observer 1 used as the baseline)
    dplyr::mutate(
      time_diff_s = Time_1 - Time_2
    ) %>% 
    dplyr::mutate(
      time_diff_ms = time_diff_s * 1000
    )

  return(
    behav_sum %>% 
      dplyr::mutate(
        recording_event = agree[i]
      ) %>% 
      dplyr::select(
        recording_event, names(.)[-grepl("recording_event", names(.))]
      )
  )

}))

agree_res %>% 
  print(n = nrow(.))

```

Summarize the time differences.
```{r}

# For 45 of the 56 or 80% behavioral states scored across these videos, we had perfect agreement in timestamps
agree_res %>% 
  dplyr::filter(time_diff_ms == 0) %>% 
  nrow()

# We disagreed on timestamps for 11 or ~20% of the behavioral states
agree_res %>%
  dplyr::filter(abs(time_diff_ms) > 0) %>% 
  nrow()

# Min, mean, and max absolute time differences (in ms) for those 11 scores
agree_res %>%
  dplyr::filter(abs(time_diff_ms) > 0) %>% 
  dplyr::summarise(
    min_diff_ms = min(abs(time_diff_ms)),
    mean_diff_ms = mean(abs(time_diff_ms)),
    max_diff_ms = max(abs(time_diff_ms))
  )

  # min_diff_ms mean_diff_ms max_diff_ms
# 1          40     215.0909         800

```

The timestamps in disagreement for behavioral states in which we had perfect agreement:
```{r}

agree_res %>%
  dplyr::filter(abs(time_diff_ms) > 0)

```

Summary of timestamps comparison: Overall, both observers have high agreement in timestamps for the behaviors scored (when we had perfect agreement in behavioral scoring). There are some instances in which our respective timestamps are pretty different from one another, from around 40 ms to up to 800 ms difference. As a reference point, finches often produce calls ~ 100 - 200ms long, so 800 ms is the equivalent of a bird producing 4 calls. We should focus on doublechecking the videos for which our timestamps disagreed by 100 ms or more:

The timestamps in disagreement by at least 100 ms:
```{r}

agree_res %>%
  dplyr::filter(abs(time_diff_ms) > 100)

```