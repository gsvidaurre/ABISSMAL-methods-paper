---
title: "Interobserver reliability"
author: "Grace Smith-Vidaurre"
date: "2024-07-18"
output: html_document
---

Purpose: Perform a validation analysis of automatically scored behaviors (e.g. behavioral inferences) obtained by ABISSMAL. The computational analyses available in ABISSMAL facilitate integrating data across movement sensors to derive coarse-grained behavioral inferences. Here, we will validate those behavioral inferences for a subsample of the full set of videos used in the methods manuscript, using a baseline dataset obtained by manual video scoring to obtain coarse-grained behavioral states for adult birds and juveniles. The current plan for this validation analysis includes 1) assessing whether the automated and manual scoring return similar levels of overall activity per unique recording event, 2) whether the automated and manual scoring return similar levels of overall activity per behavioral state in each unique recording event, 3) the degree to which the timestamps for these automated and manual scores agree with one another (this will be a bit tricky given that some of the movement sensors will detect movement before the videos), 4) we can also iterate over parameter combinations for the automated scoring to ask how these decisions influence ageement with the manual dataset.

The manually scored data needs to be concatenated and checked prior to running the final validation analysis. Also, Summer noticed some video recording events had videos switched in order, so we have to pre-process the manually scored dataset to fix these issues for the videos she caught that had this issue.

Finally, the validation analysis will be planned out and run with a random subsample of the videos used for this analysis.

```{r}

knitr::opts_knit$set(eval = TRUE, echo = TRUE)

```

```{r package and paths, warning = FALSE, message = FALSE}

# Clean the global environment
rm(list = ls())

X <- c("tidyverse", "pbapply", "data.table")

invisible(lapply(X, library, character.only = TRUE))

# Path to the aggregated BORIS spreadsheet in wide format across all video recording events
# path <- "~/Desktop/ABISSMAL_validation_data/Manual_Scoring_Results"
path <- "/home/gsvidaurre/Desktop/MANUSCRIPTS/Prep/ABISSMAL_MethodsPaper/ABISSMAL_BORIS_ManualScoringResults/Manual_Scoring_Results"

seed <- 888

```

Read in the aggregated manual scoring spreadsheet in which each row is a unique behavioral state per subject within a recording event.
```{r}

manual_scoring <- read.csv(file.path(path, "BORIS_ManualScoring_Aggregated.csv"))
glimpse(manual_scoring)

# View(manual_scoring)

# Replace dashes with underscores in the observation
manual_scoring$Observation.id <- gsub("-", "_", manual_scoring$Observation.id)

# Also replace observation IDs with duplicated "Observation"
manual_scoring$Observation.id <- gsub("Observation_Observation", "Observation", manual_scoring$Observation.id)

# Also replace observation IDs with duplicated "Observation"
manual_scoring$Observation.id <- gsub("Observatin", "Observation", manual_scoring$Observation.id)

# Which recording events are missing from this spreadsheet?
# x <- 9849 # testing
numeric_video_ids <- sapply(1:nrow(manual_scoring), function(x){
  
  tmp <- strsplit(manual_scoring$Observation.id[x], split = "_")[[1]][2]
  
  return(as.numeric(tmp))
  
}, USE.NAMES = FALSE)

# No NAs in this vector
which(is.na(unique(numeric_video_ids)))

# Which unique recording events are missing from the current aggregated spreadsheet?
all_video_ids <- seq(1, 4045, 1)

wh <- which(!all_video_ids %in% numeric_video_ids)

# These are the video recording IDs that need to be rescored and exported in aggregated format from BORIS before downstream analysis
all_video_ids[wh]

# Check that this is true
pat <- paste("Observation", all_video_ids[wh], sep = "_")
pat

# Search for this pattern in the data frame to be sure. We found no instances of these BORIS observations in the data frame, confirming that these files need to be scored/exported again
grep(paste(paste("^", pat, "$", sep = ""), collapse = "|"), manual_scoring$Observation.id)

# Export the information for videos that need to be scored/exported again
pat %>% 
  write.table(., file = file.path(path, "videos_for_aggregatedExport.txt"), sep = "\t", col.names = FALSE, row.names = FALSE, append = FALSE)

```

Next steps:

1. Summer will work on rescoring and exporting the 17 videos above from BORIS in order to have those spreadsheets in aggregated format (each row as a unique behavioral state).

2. Data pre-processing of the switched recording events that Summer identified

3. We need to choose a subsample of these videos to develop the validation analysis pipeline.

```{r}

# Randomly select 50 video recording events from the current version of the aggregated manual scoring results in wide format
set.seed(seed)
subsample <- sample(manual_scoring$Observation.id, 50, replace = FALSE)

subsample

```

4. Start writing out the steps of the validation analysis, run these with the subsample of videos chosen above

Pseudocode for the analysis:

i) Data Processing:

  - Get the manual scoring data from BORIS (in progress, nearly done)

  - Get the automated behavioral scoring used in the current methods manuscript, as well as the raw data used for these analyses

  - Check that the automated and manual datasets are in similar format

ii) Analyzing overall patterns of activity:

  - Do both scoring methods return similar levels of activity for each unique recording event?

iii) Analyzing overall patterns of activity by behavioral state:

  - Do both scoring methods return similar levels of activity for behavioral state across each unique recording event?

iv) How do the timestamps of scored behavioral states compare between methods?

*** As the best methods comparison: how badly are our statistical/biological inferences skewed using one method or the other? Iterate over parameters that we use for the automated scoring, especially the temporal thresholds used for clustering




5. Run the full validation analysis pipeline with all 4045 video recording events
