---
title: "Video validation analysis"
author: "Grace Smith-Vidaurre"
date: "2024-12-01"
output: html_document
---

Purpose: Perform a validation analysis of automatically scored behaviors (e.g. behavioral inferences) obtained by ABISSMAL. The computational analyses available in ABISSMAL facilitate integrating data across movement sensors to derive coarse-grained behavioral inferences. Here, we will validate those behavioral inferences for a subsample of the full set of videos used in the methods manuscript, using a baseline dataset obtained by manual video scoring to obtain coarse-grained behavioral states for adult birds and juveniles. The current plan for this validation analysis includes 1) assessing whether the automated and manual scoring return similar levels of overall activity per unique recording event, 2) whether the automated and manual scoring return similar levels of overall activity per behavioral state in each unique recording event, 3) the degree to which the timestamps for these automated and manual scores agree with one another (this will be a bit tricky given that some of the movement sensors will detect movement before the videos), 4) we can also iterate over parameter combinations for the automated scoring to ask how these decisions influence ageement with the manual dataset.

The manually scored data needs to be concatenated and checked prior to running the final validation analysis. Also, Summer noticed some video recording events had videos switched in order, so we have to pre-process the manually scored dataset to fix these issues for the videos she caught that had this issue.

Finally, the validation analysis will be planned out and run with a random subsample of the videos used for this analysis.

TKTK check true and false negative calculations throughout.

```{r}

knitr::opts_knit$set(eval = TRUE, echo = TRUE)

```

```{r package and paths, warning = FALSE, message = FALSE}

# Clean the global environment
rm(list = ls())

X <- c("tidyverse", "pbapply", "data.table", "tidyquant")

# Install the packages in X if not already installed
is_installed <- function(p) is.element(p, installed.packages()[,1])

invisible(lapply(1:length(X), function(x){
  if(!is_installed(X[x])){
    install.packages(X[x], repos = "http://lib.stat.cmu.edu/R/CRAN")
  }
}))

invisible(lapply(X, library, character.only = TRUE))

# Load the custom functions for computational analyses with ABISSMAL
code_path <- "~/Desktop/GitHub_repos/ABISSMAL/R"
code <- list.files(code_path, pattern = ".R$", full.names = TRUE)

invisible(lapply(1:length(code), function(i){
  source(code[i])
}))

# Path to the aggregated BORIS spreadsheet in wide format across all video recording events
path <- "~/Desktop/ABISSMAL_validation_data"

# Path to the combined raw movement sensor data
sensor_path <- file.path(path, "raw_combined")

# Initialize a folder where we will save the indexed raw data for the validation analyses
indexed_dir <- "indexed_labeled_data"
indexed_path <- file.path(path, indexed_dir)

# Create this folder if it doesn't already exist
if(!dir.exists(indexed_path)){
  dir.create(indexed_path)
}

seed <- 888

```

Initialize functions for calcuting traditional performance metrics.
```{r}

# Precision = true positives / (true positives + false positives). Out of all of the positive predictions by a method, which were truly positive? 
# In which X is a data frame in which rows correspond to unique video recording events, and columns indicating the number of true positives and false positives. The following two arguments are the column names for the true and false positives
precision <- function(X, tps_col_nm, fps_col_nm){
  # Make sure to return 0 if all values are 0 (which return NaN)
  prc <- round((X[[tps_col_nm]] / (X[[tps_col_nm]] + X[[fps_col_nm]])) * 100, 2)
  prc[is.na(prc)] <- 0
  return(prc)
}

# Sensitivity = true positives / (true positives + false negatives). This is the same as the recall, or the percentage of the true positives that a method correctly identifies
# In which X is a data frame in which rows correspond to unique video recording events, and columns indicating the number of true positives and false negatives. The following two arguments are the column names for the true and false negatives
sensitivity <- function(X, tps_col_nm, fns_col_nm){
  # Make sure to return 0 if all values are 0
  rec <- round((X[[tps_col_nm]] / (X[[tps_col_nm]] + X[[fns_col_nm]])) * 100, 2)
  rec[is.na(rec)] <- 0
  return(rec)
}

# Specificity = true negatives / (true negatives + false positives). This captures how well a method calls true negatives and avoids false positives (e.g. false alarms)
specificity <- function(X, tns_col_nm, fps_col_nm){
  # Make sure to return 0 if all values are 0
  rec <- round((X[[tns_col_nm]] / (X[[tns_col_nm]] + X[[fps_col_nm]])) * 100, 2)
  rec[is.na(rec)] <- 0
  return(rec)
}

```

Read in the aggregated manual scoring spreadsheet in which each row is a unique behavioral state per subject within a recording event.
```{r}

manual_scoring <- read.csv(file.path(path, "BORIS_ManualScoring_Aggregated_pre-processed.csv"))
glimpse(manual_scoring)

```

Read in the spreadsheet for unmasking the video file names.
```{r}

unmask_nms <- read.csv(file.path(path, "unmasked_video_names.csv"))
glimpse(unmask_nms)

```

Next steps:

1a. Data pre-processing of the switched recording events that Summer identified. Look at the spreadsheet called "BORIS_VideoList_TimeFlipped.csv". We could i) exclude these videos from analyses, or ii) we could try to flip the manual scores given the timestamps of the scores and the videos themselves, or iii) if one video has extremely short duration, then drop the scores for that video in the pair. TKTK keep thinking about a solution

1b. Figure out if we can discriminate videos for which video recording was initialized by daily animal checks (a hand opening the nest container), check the notes column of the aggregated .csv. This will be scored as no birds inside of the containr but may have a very high pixel number that changed. Summer kept track in her log, but this only happened 2-3 times, check the notes of the BORIS .csv, recording event 2322 

2. We need to choose a subsample of these videos to develop the validation analysis pipeline.

```{r}

# Randomly select 50 video recording events from the current version of the aggregated manual scoring results in wide format

# First get all of the unique BORIS Observation IDs that correspond to unique recording events
unique_obs_ids <- unique(manual_scoring$Observation.id)
head(unique_obs_ids)

# There should be 4045 Observation IDs to match the unique recording events that Summer scored, looks good
length(unique_obs_ids)

set.seed(seed)
subsample <- sample(unique_obs_ids, 50, replace = FALSE)

subsample

```

3. Start writing out the steps of the validation analysis, run these with the subsample of videos chosen above

Pseudocode for the analysis:

i) Data Processing:

- Get the manual scoring data from BORIS in aggregated format (done)

- Decide how to handle the time-flipped videos (in progress)

- Unmask the video file names used for manual scoring

```{r}

manual_scoring_ss <- manual_scoring %>% 
  dplyr::filter(Observation.id %in% subsample)

dim(manual_scoring_ss)

# Checking filtering, looks good
# all(unique(manual_scoring_ss$Observation.id) %in% subsample)
# all(subsample %in% unique(manual_scoring_ss$Observation.id))

# Add the unmasked video file names to each observation in the manual scoring spreadsheet
# i <- 1 # testing
manual_scoring_nm <- data.table::rbindlist(pblapply(1:nrow(manual_scoring_ss), function(i){
  
  # Get the masked video recording event ID
  masked_nm <- manual_scoring_ss$masked_video_recording_event_ID[i]
  # masked_nm
  
  # Use the masked video recording event ID to find the unmasked recording event ID per row
  unmasked_nm <- unmask_nms %>% 
    dplyr::filter(masked_name == masked_nm) %>% 
    pull(videos)
  
  # Add the unmasked name back to the manual scoring data frame
  # The unmasked name will be one of the two unique video files per unique recording event (we don't need both names for later steps)
  return(
    manual_scoring_ss[i, ] %>% 
      dplyr::mutate(
        unmasked_video_file = unmasked_nm
      )
  )
  
}))

# Select a few rows at random and manually check the unmasking results
glimpse(manual_scoring_nm)
dim(manual_scoring_nm)
# View(manual_scoring_nm)

```

Get the raw data across movement sensors used for analyses in the current methods manuscript.
```{r}

# The movement sensor data that we are using here is the raw data that we have used in the methods manuscript, and it's already been combined across days of data collection using ABISSMAL functions. We have one spreadsheet per sensor type, and the beam breaker spreadsheet contains data for 2 pairs of beam breakers
list.files(sensor_path)

# We will use this data to index the raw movement sensor data by the timestamps of the videos that Summer scored. The spreadsheet for the video recording events contains timestamp columns that correspond to the time at which the video camera detected motion and began recording the post-movement video (this timestamp is also in the filenames of each of the videos per unique recording event).
video_events <- read.csv(file.path(sensor_path, "combined_raw_data_Video.csv")) %>% # Make sure that the timestamps are in the right format
  dplyr::mutate(
    timestamp_ms = as.POSIXct(format(as.POSIXct(timestamp_ms, tz = ""), "%Y-%m-%d %H:%M:%OS6")))

glimpse(video_events)

# View(video_events)

# In the BORIS manual scoring data, we can use the column of Media File Duration to extract the exact duration of each video in each unique recording event (lots of variability in the duration of the pre-movement video recorded using the ring buffer)

```

Use the unmasked video file names to get the start timestamp of the post-movement video in each unique recording event in the spreadsheet of video recording events recorded by ABISSMAL. In this metadata spreadsheet for the video recordings he column timestamp_ms that holds the time when the camera detected motion and started recording video. Use these timestamps along with the durations of the pre- and post-movement videos from the aggregated BORIS manual scoring spreadsheet to obtain the start and end timestamps of each unique recording event (the pre- and post-movement videos together).
```{r}

# Get the unique prefixes for unmasked video recording event names
unique_video_pats <- manual_scoring_nm %>% 
  pull(unmasked_video_file) %>% 
  gsub("pre_trigger.mp4|_post_trigger.mp4", "", .) %>% 
  unique()

unique_video_pats

# Filter the video recording event metdata spreadsheet to obtain the start timestamps for the post-movement video per unique recording event
# i <- 1
recording_event_ts <- data.table::rbindlist(pblapply(1:length(unique_video_pats), function(i){
  
  # Get the start timestamp for the post-movement video of the current unique recording event
  start_post <- video_events %>% 
    dplyr::filter(grepl(unique_video_pats[i], video_file_name)) %>% 
    pull(timestamp_ms) %>% 
    unique()
  
  # Get the duration of each video file in the given recording event
  tmp <- manual_scoring_nm %>% 
    dplyr::filter(grepl(unique_video_pats[i], unmasked_video_file)) %>% 
    separate(
      ., col = "Media.duration..s.", into = c("video_1_dur", "video_2_dur"), sep = ";", remove = FALSE
    ) %>% 
    dplyr::select(c("video_1_dur", "video_2_dur")) %>% 
    distinct() %>%
    # Convert the duration in seconds to numeric
    dplyr::mutate(
      video_1_dur = as.numeric(video_1_dur),
      video_2_dur = as.numeric(video_2_dur)
    )
  
  # Create the start timestamp for this unique recording event by subtracting the duration of the first video in this event from the timestamp of the post-motion video
  start_event <- start_post - tmp$video_1_dur
  # start_event
  
  # Create the end timestamp for this unique recording event by adding the duration of the second video to the start timestamp of the post-motion video
  end_event <- start_post + tmp$video_2_dur
  # end_event
  
  # Check that the duration of these timestamps yields the same duration as the sum of the media durations in the BORIS spreadsheet
  offset <- as.numeric(end_event - start_event) - (tmp$video_1_dur + tmp$video_2_dur)
  
  if(offset < 0.01){
    
    return(
      data.frame(
        unique_recording_event_ID = unique_video_pats[i],
        start_event = start_event,
        end_event = end_event
      )
    )
    
  } else {
    
    stop("The video durations do not line up")
    
  }
  
}))

# We can use the timestamps to index the raw detections for all other movement sensors for validation
glimpse(recording_event_ts)
# View(recording_event_ts)

# The same number of rows should be here compared to the number of videos in the subsample
nrow(recording_event_ts) == length(subsample)

```

Read in the combined raw data for each sensor type.
```{r}

# Read in the raw combined infrared beam breaker detections
irbb_events <- read.csv(file.path(sensor_path, "combined_raw_data_IRBB.csv")) %>% # Make sure that the timestamps are in the right format
  dplyr::mutate(
    timestamp_ms = as.POSIXct(format(as.POSIXct(timestamp_ms, tz = ""), "%Y-%m-%d %H:%M:%OS6")))

glimpse(irbb_events)

# Read in the raw combined RFID detections
rfid_events <- read.csv(file.path(sensor_path, "combined_raw_data_RFID.csv")) %>% # Make sure that the timestamps are in the right format
  dplyr::mutate(
    timestamp_ms = as.POSIXct(format(as.POSIXct(timestamp_ms, tz = ""), "%Y-%m-%d %H:%M:%OS6")))

glimpse(rfid_events)

# Read in the raw combined video detections
video_events <- read.csv(file.path(sensor_path, "combined_raw_data_Video.csv")) %>% # Make sure that the timestamps are in the right format
  dplyr::mutate(
    timestamp_ms = as.POSIXct(format(as.POSIXct(timestamp_ms, tz = ""), "%Y-%m-%d %H:%M:%OS6")))

glimpse(video_events)

```

The next step will be to perform automated behavioral scoring of the raw movement sensor data using the default temporal thresholds for automated data processing and behavioral inference from ABISSMAL computational analyses.

# Perching events

Find RFID and beam breaker perching events. Iterate over different temporal thresholds used to label stretches of detections as perching events.
```{r find perching events}

ths <- c(0.1, 0.2, 1, 2, 4) # temporal thresholds in seconds

invisible(pblapply(1:length(ths), function(thx){
  
  detect_perching_events(file_nm = "combined_raw_data_RFID.csv", threshold = ths[thx], run_length = 2, sensor_id_col_nm = "sensor_id", timestamps_col_nm = "timestamp_ms", PIT_tag_col_nm = "PIT_tag_ID", rfid_label = "RFID", outer_irbb_label = NULL, inner_irbb_label = NULL, general_metadata_cols = c("chamber_id", "sensor_id"), path = path, data_dir = "raw_combined", out_dir = "processed", out_file_prefix = paste("perching_events_th-", ths[thx], sep = ""), tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")
  
}))

invisible(pblapply(1:length(ths), function(thx){
  
  detect_perching_events(file_nm = "combined_raw_data_IRBB.csv", threshold = ths[thx], run_length = 2, sensor_id_col_nm = "sensor_id", timestamps_col_nm = "timestamp_ms", PIT_tag_col_nm = NULL, rfid_label = NULL, outer_irbb_label = "Outer Beam Breaker", inner_irbb_label = "Inner Beam Breaker", general_metadata_cols = c("chamber_id", "sensor_id"), path = path, data_dir = "raw_combined", out_dir = "processed", out_file_prefix = paste("perching_events_th-", ths[thx], sep = ""), tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")
  
}))

```

# Pre-processing raw data

Pre-process the RFID, beam breaker, and video data for the indexed detections per sensor. Iterate over different temporal thresholds used to remove beam breaker and RFID detections that were very close together.

TKTK update code, need to either update the function below to include a prefix, or instead run all functions together per temporal threshold and save these in separate directories by threshold value.
```{r preprocess data}

ths <- c(0.1, 0.2, 1, 2, 4) # temporal thresholds in seconds

invisible(pblapply(1:length(ths), function(thx){
  
  # Used thinning with a threshold of 2 seconds for beam breakers as well
  preprocess_detections(sensor = "IRBB", timestamps_col_nm = "timestamp_ms", group_col_nm = "sensor_id", pixel_col_nm = NULL, mode = "thin", thin_threshold = 2, pixel_threshold = NULL, drop_tag = NULL, path = path, data_dir = "raw_combined", out_dir = "processed", tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")
  
}))

# Performed pre-processing with a threshold of 2 seconds for RFID because I removed the timestamp difference rounding
# The PIT tag IDs specified in `drop_tag` were tags I used for testing the RFID antenna and were not used to mark birds
invisible(pblapply(1:length(ths), function(thx){
  
  preprocess_detections(sensor = "RFID", timestamps_col_nm = "timestamp_ms", group_col_nm = "PIT_tag_ID", pixel_col_nm = NULL, mode = "thin", thin_threshold = 2, pixel_threshold = NULL, drop_tag = c("01-10-3F-84-FC", "01-10-16-B8-7F"), path = path, data_dir = "raw_combined", out_dir = "processed", tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")
  
}))

# Used a pixel threshold of 1000 for video data. Since we used 9000 pixels as the sensitivity threshold for motion detection, this filtering threshold will not drop any data
# This filtering should not drop any videos in the indexed data, and looks good, 100 rows remain
preprocess_detections(sensor = "Video", timestamps_col_nm = "timestamp_ms", group_col_nm = NULL, pixel_col_nm = "total_pixels_motionTrigger", mode = NULL, thin_threshold = NULL, pixel_threshold = 1000, drop_tag = NULL, path = path, data_dir = "raw_combined", out_dir = "processed", tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")

```

# Integrate pre-processed data

Find clusters of detections across all 3 movement sensor that represent movement events per.
```{r find detection clusters for 3 sensor types}

# Using camera_label = "Camera" again here since that's was carried through from the raw data
# The run length needs to be set to 1 in order to correctly detect detection clusters of length 2
detect_clusters(file_nms = c("pre_processed_data_IRBB.csv", "pre_processed_data_RFID.csv", "pre_processed_data_Video.csv"), threshold = 2, run_length = 1, sensor_id_col_nm = "sensor_id", timestamps_col_nm = "timestamp_ms", PIT_tag_col_nm = "PIT_tag_ID", rfid_label = "RFID", camera_label = "Camera", preproc_metadata_col_nms = c("thin_threshold_s", "pixel_threshold", "data_stage", "date_pre_processed"), general_metadata_col_nms = c("chamber_id", "year", "month", "day"), video_metadata_col_nms = c("total_pixels_motionTrigger", "video_file_name"), path = path, data_dir = "processed", out_dir = "processed", out_file_nm = "detection_clusters.csv", tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")

```

Score detection clusters to make inferences about behavioral events associated with these movements.  `score_clusters` should be making inferences by just the first edge encountered, not all edges in a sequence, because the system is currently targeted to capturing overall patterns of activity.
```{r score detection clusters for 3 sensor types}

score_clusters(file_nm = "detection_clusters.csv", rfid_label = "RFID", camera_label = "Camera", outer_irbb_label = "Outer Beam Breaker", inner_irbb_label = "Inner Beam Breaker", video_metadata_col_nms = c("total_pixels_motionTrigger", "video_file_name"), integrate_perching = TRUE, perching_dataset = "RFID-IRBB", perching_prefix = "perching_events_", sensor_id_col_nm = "sensor_id", PIT_tag_col_nm = "PIT_tag_ID", pixel_col_nm = "total_pixels_motionTrigger", video_width = 1280, video_height = 720, integrate_preproc_video = TRUE, video_file_nm = "pre_processed_data_Video.csv", timestamps_col_nm = "timestamp_ms", path = path, data_dir = "processed", out_dir = "processed", out_file_nm = "scored_detectionClusters.csv", tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")

```

# Index the automatatically scored data by the buffered timestamps of the manually scored videos

Index the raw data per movement sensor type by the start and end timestamps of all of the videos used for manual scoring for validation.
```{r}

# Iterate over rows in the video recording event timestamps to pull out raw data from each of the movement sensor types (including the videos themselves), using a buffer of 2 seconds before and after the start and end timestamps of each unique video recording event

scored_clusters <- read.csv(file.path(path, "processed", "scored_detectionClusters.csv"))  %>% 
  # Make sure that the timestamps are in the right format
  dplyr::mutate(
    start = as.POSIXct(format(as.POSIXct(start, tz = ""), "%Y-%m-%d %H:%M:%OS6")),
    end = as.POSIXct(format(as.POSIXct(end, tz = ""), "%Y-%m-%d %H:%M:%OS6"))
  )

glimpse(scored_clusters)

# Initialize the temporal buffer for indexing (seconds)
buf <- 2

# Testing
# i <- 1

indexed_scores <- invisible(data.table::rbindlist(lapply(1:nrow(recording_event_ts), function(i){
  
  tmp <- scored_clusters %>% 
    dplyr::filter(
      start >= recording_event_ts$start_event[i] - buf & 
        end <= recording_event_ts$end_event[i] + buf
    )
  
  return(tmp)
  
})))

glimpse(indexed_scores)

indexed_scores %>%
  write.csv(file.path(indexed_path, "indexed_scored_clusters.csv"), row.names = FALSE)

```

Next, we need to check that the automated and manual datasets are in similar format in order to make a 1:1 plot for the very first validation figure.
```{r}

auto_scoring <- read.csv(file.path(indexed_path, "indexed_scored_clusters.csv")) %>%
  # Make sure that the timestamps are in the right format
  dplyr::mutate(
    start = as.POSIXct(format(as.POSIXct(start, tz = ""), "%Y-%m-%d %H:%M:%OS6")),
    end = as.POSIXct(format(as.POSIXct(end, tz = ""), "%Y-%m-%d %H:%M:%OS6"))
  )

# Check out the automated inferences and the manual scoring results (filtered by the subsample of videos used to develop this code, and unmasked)
# glimpse(auto_scoring)
# glimpse(manual_scoring_nm)

# View(auto_scoring)
# View(manual_scoring_nm)

# These columns contain the name of the video file associated with each event that was scored. The automated inferences will have NAs in this column when a video recording event was not associated with the given event
# auto_scoring$video_file_name
# manual_scoring_nm$unmasked_video_file

# The manual scoring protocol was oriented to score several behavioral states (and per subject) across each video recording event, while the automated scoring can identify multiple behavioral events associated with each video recording event, but is less likely to identify as many as the manual scoring given that we have more limited individual resolution (RFID at the container entrance only, and this can fail to detect PIT tags). In the automated inferences, are there cases when a single video recording event was used for more than one behavioral score?

# Yes, although in this subsample of the data there is only 1 video recording event that meets this condition
# length(auto_scoring$video_file_name[!is.na(auto_scoring$video_file_name)])
# length(unique(auto_scoring$video_file_name[!is.na(auto_scoring$video_file_name)]))
# which(duplicated(auto_scoring$video_file_name[!is.na(auto_scoring$video_file_name)]))
# auto_scoring$video_file_name[!is.na(auto_scoring$video_file_name)][3]
# Box_03_2023_7_22_13_24_42_pre_trigger.mp4

# auto_scoring %>% 
# dplyr::filter(video_file_name == "Box_03_2023_7_22_13_24_42_pre_trigger.mp4") %>% 
# View()

# How many automated scores have no associated video?
# Two rows
auto_scoring %>% 
  dplyr::filter(is.na(video_file_name)) %>% 
  nrow()

# Iterate over rows in the automated scores, and for scored events not associated with videos, figure out whether those timestamps align with the start and end of a unique recording event with the added time buffer on either end
auto_scoring_extraAssignedVids <- data.table::rbindlist(lapply(1:nrow(auto_scoring), function(i){
  
  # cat(paste("i = ", i, "\n", sep = " "))
  
  tmp_auto <- auto_scoring %>% 
    slice(i)
  
  if(is.na(tmp_auto$video_file_name)){
    
    # This filter will only find matches when the non-video sensor event started before the video recording
    tmp <- recording_event_ts %>% 
      # Filter using the start time, and bound the end timestamp to fall within 18 seconds after the start (to account for recording events composed of a video of ~6 seconds, then a video of ~ 10 seconds, sometimes up to 18 seconds together)
      dplyr::filter(
        start_event >= tmp_auto$start & end_event <= (tmp_auto$start + 18)
      )
    
    # Perform a search for events that occurred in the middle of a video recording if the search above didn't return results
    if(nrow(tmp) == 0){
      
      tmp <- recording_event_ts %>%
        # This filter will only find matches when the non-video sensor event started after the video recording
        dplyr::filter(
          tmp_auto$start >= start_event & tmp_auto$start <= end_event
        )
      
    }
    
    if(nrow(tmp) > 0){
      
      # Return the current row with the video recording event assigned as above
      res <- tmp_auto %>% 
        dplyr::mutate(
          # Add the _pre-trigger suffix to match how videos within a video recording event pair were assigned during ABISSMAL automated analysis
          assigned_recording_event = paste(tmp$unique_recording_event_ID, "pre_trigger.mp4", sep = "")
        )
      
    } else {
      
      # Return the current row with the video recording event assigned as missing
      res <- tmp_auto %>% 
        dplyr::mutate(
          assigned_recording_event = NA
        )
      
    }
    
  } else {
    
    res <- tmp_auto %>% 
      dplyr::mutate(
        assigned_recording_event = video_file_name
      )
    
  }
  
  return(res)
  
}))


glimpse(auto_scoring_extraAssignedVids)

# All values that are missing the suffix "pre-trigger" or "post-trigger" were assigned in the loop above
unique(auto_scoring_extraAssignedVids$assigned_recording_event)

checking <- paste(recording_event_ts$unique_recording_event_ID, "pre_trigger.mp4", sep = "")

# Currently there are not more videos assigned to the automated detections than there are in the subsample of manually scored videos, looks good
auto_scoring_extraAssignedVids$assigned_recording_event[!grepl(paste(checking, collapse = "|"), auto_scoring_extraAssignedVids$assigned_recording_event)]

# Then we need to summarize the automated and manually scored data to yield a single row per video recording event, in order to make a 1:1 plot. In this plot, there should be more behavioral states scored manually than using the automated inferences (see my rationale above)
auto_scoring_summ <- auto_scoring_extraAssignedVids %>%
  # Drop rows that did not have an associated video recording event
  dplyr::filter(!is.na(video_file_name)) %>% 
  group_by(video_file_name) %>% 
  dplyr::summarise(
    n_events = n()
  )

manual_scoring_nm_summ <- manual_scoring_nm %>% 
  group_by(unmasked_video_file) %>% 
  dplyr::summarise(
    n_events = n()
  )

glimpse(auto_scoring_summ)
glimpse(manual_scoring_nm_summ)

# Merge this data together and make a plot
gg_res <- auto_scoring_summ %>% 
  dplyr::mutate(
    type = "automated_inference"
  ) %>% 
  bind_rows(
    manual_scoring_nm_summ %>%
      dplyr::rename(
        `video_file_name` = "unmasked_video_file"
      ) %>% 
      dplyr::mutate(
        type = "manual_scoring"
      )
  )

# This result makes sense given the coarser-grained temporal resolution of the automated scoring 
gg_res %>% 
  pivot_wider(id_cols = video_file_name, names_from = type, values_from = "n_events") %>%
  dplyr::arrange(automated_inference, manual_scoring) %>% 
  ggplot(aes(x = manual_scoring, y = automated_inference)) +
  geom_point() +
  geom_jitter(width = 0.25, height = 0.25) +
  geom_abline(slope = 1, linetype = "dotted") +
  scale_y_continuous(limits = c(0, max(gg_res$n_events)), breaks = seq(0, max(gg_res$n_events), 1)) +
  scale_x_continuous(limits = c(0, max(gg_res$n_events)), breaks = seq(0, max(gg_res$n_events), 1)) +
  xlab("Behavioral events obtained by manual scoring") +
  ylab("Behavioral events obtained by automated inference") +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank()
  )


```

Next steps:

- Start validation analyses at a very coarse-grained scale with the manual scoring as a baseline:

1. OVERALL ACTIVITY DETECTION: First, how well does the integration of 3 movement sensors capture whether birds are inside of the container or not? Can we generally use the automated scores to infer that sensor activity was due to the birds or not? For this analysis, perform integration of data from the 3 sensors for all days of data collection, and then index the scored activities using the timestamps of the manually scored video recording events with a time buffer around them. 

General approach to calculating detection performance:

For the binary analysis below, we want to know how often ABISSMAL and manual scoring picked up activity versus no activity, regardless of the behavior itself, the individual, or the location (container entrance or inside of the container). Throughout this analysis, keep in mind that the manually scored dataset relies on videos recorded by motion detection through ABISSMAL, and therefore we do not have access to a manually scored dataset of continuous video recordings, which would let us address whether activity occurred or not outside of the timestamps of the ABISSMAL automated scores. Later validation work to truly assess the metrics below will require manual scoring of continuous video recordings, accompanied by automated scoring through ABISSMAL (including a separate stream of motion-detection video recordings).

**True positive** = activity was detected by both ABISSMAL and manual scoring. How often did ABISSMAL capture activity when activity was also picked up by manual scoring (any of PERC, ENTR, EXIT, INSC captured by both methods)?

**False negative** = ABISSMAL does not detect activity, but manual scoring detected activity. Currently we cannot assess true negatives for this analysis because the current version of the ABISSMAL functions only infer activity, not the lack of activity.

**False positive** = ABISSMAL detects activity, but manual scoring detected a lack of activity. How often did manual scoring find no birds inside of the container for the duration of the video recording event (NOBR) but ABISSMAL detected activity (any of PERC, ENTR, EXIT, INSC)?

**True negative** = neither ABISSMAL nor manual scoring detect activity. We cannot calculate true negatives for the same reasons as above with false negatives.

Overall, we can assess the rate of true positives and the rate of false positives, which facilitates calculating the precision. This metric represents the percentage of the automatically inferred activity labels that were truly positive (e.g. that corresponded to activity scored by a human observer): 

true positives / (true positives + false positives)

```{r}

# First we need to condense the behavioral inferences from automated scoring into a single column
auto_scoring2 <- auto_scoring_extraAssignedVids %>%
  dplyr::mutate(
    perching_inference = ifelse(!is.na(perching_rfid_start) | !is.na(perching_outer_irbb_start) | !is.na(perching_inner_irbb_start), "perching", NA)) %>% 
  dplyr::mutate(
    inferences = paste(direction_scored, inferredMovement_Location, perching_inference, sep = "-")
  )

glimpse(auto_scoring2)

table(auto_scoring2$inferences)

# no perching events represented here with this video subsample:

# entrance-container_entrance-NA      ### this is an entrance or ENTR
#                              6 
# exit-container_entrance-NA          ### this is an exit or EXIT
#                              3 
# NA-inside_container-NA              ### this is inside_container or INSC
#                             44 

# Will need to add back perching events for the full dataset (currently that inference is NA here)
auto_scoring2 <- auto_scoring2 %>% 
  dplyr::mutate(
    inferences = ifelse(inferences == "entrance-container_entrance-NA", "ENTR", inferences),
    inferences = ifelse(inferences == "exit-container_entrance-NA", "EXIT", inferences),
    inferences = ifelse(inferences == "NA-inside_container-NA", "INSC", inferences)
  )

# Looks good
table(auto_scoring2$inferences)
glimpse(auto_scoring2)

```

Then we need to condense the automated scoring and the manual scoring data frame to have a single row per video.
```{r}

# View(manual_scoring_nm)
names(manual_scoring_nm)

# Get one row of behavioral labels per video recording event
manual_table <- manual_scoring_nm %>% 
  group_by(unmasked_video_file) %>%
  # Create a unique row identifier within videos
  dplyr::mutate(
    row_id = row_number()
  ) %>% 
  ungroup() %>% 
  # View()
  dplyr::select(row_id, unmasked_video_file, Behavior) %>% 
  pivot_wider(names_from = unmasked_video_file, values_from = Behavior) %>% 
  t()

auto_table <- auto_scoring2 %>% 
  group_by(assigned_recording_event) %>%
  # Create a unique row identifier within videos
  dplyr::mutate(
    row_id = row_number()
  ) %>% 
  ungroup() %>% 
  # View()
  dplyr::select(row_id, assigned_recording_event, inferences) %>% 
  pivot_wider(names_from = assigned_recording_event, values_from = inferences) %>% 
  t()

# View(auto_scoring2)

View(auto_table)
View(manual_table)

```

**True positive** = activity was detected by both ABISSMAL and manual scoring. How often did ABISSMAL capture activity when activity was also picked up by manual scoring (any of PERC, ENTR, EXIT, INSC captured by both methods)?

**False positive** = ABISSMAL detects activity, but manual scoring detected a lack of activity. How often did manual scoring find no birds inside of the container for the duration of the video recording event (NOBR) but ABISSMAL detected activity (any of PERC, ENTR, EXIT, INSC)?

Then: precision = true positives / (true positives + false positives)

```{r}

# Looks good
length(unique(auto_scoring2$assigned_recording_event))
length(unique(manual_scoring_nm$unmasked_video_file))

# Iterate over all the unique videos included in the automated and manual scoring tables to find true positives and false negatives
unique_videos <- unique(manual_scoring_nm$unmasked_video_file)
unique_videos

glimpse(auto_table)
glimpse(manual_table)

activity_labels <- c("PERC", "ENTR", "EXIT", "INSC")
no_activity_label <- "NOBR"

i <- 1

tp_fp <- data.table::rbindlist(pblapply(1:length(unique_videos), function(i){
  
  # Get unique behavioral scores for the given unique recording event from summarized tables of both scoring methods
  auto_scores <- unique(auto_table[grep(unique_videos[i], dimnames(auto_table)[[1]]), ])
  
  manual_scores <- unique(manual_table[grep(unique_videos[i], dimnames(manual_table)[[1]]), ])
  
  # Convert scores to Boolean using the activity labels
  auto_a <- ifelse(any(activity_labels %in% auto_scores), TRUE, FALSE)
  manual_a <- ifelse(any(activity_labels %in% manual_scores), TRUE, FALSE)
  
  # Convert scores to Boolean using the no activity label
  auto_n <- ifelse(any(no_activity_label %in% auto_scores), TRUE, FALSE)
  manual_n <- ifelse(any(no_activity_label %in% manual_scores), TRUE, FALSE)
  
  # auto_a
  # manual_a
  # auto_n
  # manual_n
  
  # Score true positives: both methods identitfied a type of activity
  if(auto_a & manual_a){
    
    true_positive <- 1
    
  } else {
    
    true_positive <- 0
    
  }
  
  # Score false positives: manual scoring identified NOBR but ABISSMAL detected a type of activity
  if(!manual_n & auto_n){
    
    false_positive <- 1
    
  } else {
    
    false_positive <- 0
    
  }
  
  return(
    data.frame(
      unique_video_recording_event = unique_videos[i],
      true_positive = true_positive,
      false_positive = false_positive
    )
  )
  
}))


glimpse(tp_fp)

```

Calculate the precision of the overall activity detection, which for the current subsample of 50 video recording events was 100%.
```{r}

# Precision is: true positives / (true positives + false positives)

tp_fp_sum <- tp_fp %>% 
  dplyr::summarise(
    tps = sum(true_positive),
    fps = sum(false_positive)
  )

precision_oa <- precision(X = tp_fp_sum, tps_col_nm = "tps", fps_col_nm = "fps")
precision_oa

```


2. LOCATION INFERENCE: Next, focus on asking whether the automated and manual scores label activity by birds at the container entrance and/or inside of the container. The options here are: container entrance, inside or container, or both. For this analysis, we will use any activity score between the two methods (any of PERC, ENTR, EXIT, INSC), but ask whether the two methods agree in the location of activity. We will also focus only on manual scores of activity, and drop manual scores of no activity (NOBR).

Generall approach to calculating detection performance:

**True positive** = activity was detected at the same location(s) by both ABISSMAL and manual scoring. How often did ABISSMAL capture activity in the same location when activity at the same location was also picked up by manual scoring (any activity label, and both methods capture activity at the same location)?

**False negative** = ABISSMAL does not detect activity at a given location(s) where manual scoring detects activity. 

**False positive** = ABISSMAL detects activity at a given location(s), but manual scoring does not.

**True negative** = Neither ABISSMAL nor manual scoring detect activity for a given location(s).

Overall, we can assess both the precision (out of the positive predictions, which are truly positive) and sensitivity (or recall, what percentage of the total positives were predicted) of activity location inferences by ABISSMAL, as well as the the specificity (true negative rate, or how well the true negatives are predicted while avoiding false alarms). Note that false negatives and positives will be the same because they are defined as a mismatch in location scores between methods.

We need to calculate true positives and false negatives, and then each of the performance metrics, for each of the location classes. 
```{r}

# Iterate over all the unique videos included in the automated and manual scoring tables to find true positives and false negatives
# Also remove video recording events that had a NOBR score only
# unique(manual_scoring_nm$Behavior)

unique_videos <- manual_scoring_nm %>% 
  dplyr::filter(!grepl("NOBR", Behavior)) %>% 
  pull(unmasked_video_file) %>% 
  unique()

unique_videos

glimpse(auto_table)
glimpse(manual_table)

# container entrance is "PERC", "ENTR", "EXIT"
# inside of container is "INSC"
# it is also possible to score activity at both locations

# both is "PERC|ENTR|EXIT" and "INSC" 
locations <- c("container_entrance", "inside_container", "both")
labels <- list(
  "container_entrance" = c("PERC", "ENTR", "EXIT"),
  "inside_container" = c("INSC"),
  "both" = c("PERC|ENTR|EXIT", "INSC")
)

labels

# testing
# i <- 1
# n <- 3

tp_fp_tn_fn <- data.table::rbindlist(pblapply(1:length(unique_videos), function(i){
  
  # Get unique behavioral scores for the given unique recording event from summarized tables of both scoring methods
  auto_scores <- unique(auto_table[grep(unique_videos[i], dimnames(auto_table)[[1]]), ])
  auto_scores <- auto_scores[!is.na(auto_scores)]
  
  manual_scores <- unique(manual_table[grep(unique_videos[i], dimnames(manual_table)[[1]]), ])
  manual_scores <- manual_scores[!is.na(manual_scores)]
  
  # Iterate over locations
  res <- data.table::rbindlist(lapply(1:length(locations), function(n){
    
    # Convert scores to Boolean using the labels associated with the respective location
    if(!locations[n] %in% "both"){
      
      auto <- ifelse(any(labels[[locations[n]]] %in% auto_scores), TRUE, FALSE)
      manual <- ifelse(any(labels[[locations[n]]] %in% manual_scores), TRUE, FALSE)
      
      # If checking for labels in both locations, then loop over the location labels for a thorough Boolean conversion
    } else {
      
      tmp_labs <- labels[[locations[n]]]
      
      tmp_res <- data.table::rbindlist(lapply(1:length(tmp_labs), function(z){
        
        auto <- any(grepl(tmp_labs[z], auto_scores))
        manual <- any(grepl(tmp_labs[z], manual_scores))
        
        return(
          data.frame(
            auto = auto,
            manual = manual
          )
        )
        
      }))
      
      auto <- all(tmp_res$auto)
      manual <- all(tmp_res$manual)
      
    }
    
    # Score true positives
    if(auto & manual){
      
      true_positive <- 1
      
    } else {
      
      true_positive <- 0
      
    }
    
    # Score false positives
    if(auto & !manual){
      
      false_positive <- 1
      
    } else {
      
      false_positive <- 0
      
    }
    
    # Score true negatives
    if(!manual & !auto){
      
      true_negative <- 1
      
    } else {
      
      true_negative <- 0
      
    }
    
    # Score false negatives
    if(manual & !auto){
      
      false_negative <- 1
      
    } else {
      
      false_negative <- 0
      
    }
    
    return(
      data.frame(
        unique_video_recording_event = unique_videos[i],
        location = locations[n],
        true_positive = true_positive,
        false_positive = false_positive,
        true_negative = true_negative,
        false_negative = false_negative
      )
    )
    
  }))
  
  return(res)
  
}))

# There should be 3 rows per unique video recording event, looks good
glimpse(tp_fp_tn_fn)
# View(tp_fp_tn_fn)

```

Calculate precision, recall, sensitivity, and specificity of activity detection by location.
```{r}

tp_fp_tn_fn_sum <- tp_fp_tn_fn %>% 
  group_by(location) %>% 
  dplyr::summarise(
    tps = sum(true_positive),
    fps = sum(false_positive),
    tns = sum(true_negative),
    fns = sum(false_negative)
  ) %>% 
  ungroup()

tp_fp_tn_fn_sum

# Precision across locations
precision_loc <- sapply(1:length(locations), function(i){
  
  precision(X = tp_fp_tn_fn_sum[grep(locations[i], tp_fp_tn_fn_sum$location), ], tps_col_nm = "tps", fps_col_nm = "fps")
  
})

# names(precision_loc) <- locations
precision_loc

# Sensitivity across locations
sensitivity_loc <- sapply(1:length(locations), function(i){
  
  sensitivity(X = tp_fp_tn_fn_sum[grep(locations[i], tp_fp_tn_fn_sum$location), ], tps_col_nm = "tps", fns_col_nm = "fns")
  
})

sensitivity_loc

# Specificity across locations
specificity_loc <- sapply(1:length(locations), function(i){
  
  specificity(X = tp_fp_tn_fn_sum[grep(locations[i], tp_fp_tn_fn_sum$location), ], tns_col_nm = "tps", fps_col_nm = "fns")
  
})

specificity_loc

# Note that sensitivity and specificity are the same, given the way that false positives and negatives were defined

data.frame(
  locations = locations,
  precision = precision_loc,
  sensitivity = sensitivity_loc,
  specificity = specificity_loc
)

```


4. RFID DETECTION: Next, ask how reliably the RFID system that ABISSMAL depends on for individual identification captured activity by the PIT-tagged adults compared to manual scoring of subjects. In this analysis, we will focus on asking how often the PIT tags of adults were detected compared to the detection of adult subjects by manual scoring. This is an important coarse-grained validation of the RFID system prior to the finer-grained individual identity validation, because there were non-PIT-tagged birds in the nest container later in development (juveniles) and also because it is common for RFID systems to fail to detect PIT tags moving through RFID antennae.

General approach to calculating RFID detection performance:

**True positive** = Activity by adults was detected by both ABISSMAL (PIT tag detected) and manual scoring (adult subjects detected).

**False negative** = ABISSMAL detected no PIT tag but adult subjects were detected by manual scoring.

**False positive** = ABISSMAL detected a PIT tag but no adult subjects were detected by manual scoring.

**True negative** = Neither ABISSMAL nor manual scoring detected activity by adult birds (no PIT tags detected by ABISSMAL, and no adult subjects detected by manual scoring).

Assess the precision, sensitivity, and specificity of the RFID module used by ABISSMAL.

First we need to condense the automated scoring and the manual scoring data frames to have a single row per video while retaining information about adult activity (PIT tags in the ABISSMAL data, and adult subjects in the manually scored data).
```{r}

# View(manual_scoring_nm)
names(manual_scoring_nm)


########### Convert the subject labels per dataset

# The subject names for adults in the manually scored data are "Female", "Male", "Unknown_Adult"
# Subject names for no detections of adults will be all other subject labels: "Juveniles_Pooled", "Juvenile", "No_Birds", and "Bird_of_Unknown_Age" (to be conservative, since such subjects could have been juvenile)

# The unique PIT tag IDs for adults in this dataset are: "01-10-3F-8F-F0" and "01-10-16-CD-1C" 
# unique(scored_clusters$individual_initiated)

# Convert the subject labels in the manually scored dataset to binary labels: Adult or No Adult
manual_scoring_nm2 <- manual_scoring_nm %>% 
  # Replace the subject labels to be "Adult" or "No Adult"
  dplyr::mutate(
    Subject_binary = ifelse(Subject %in% c("Female", "Male", "Unknown_Adult"), "Adult", Subject),
    Subject_binary = ifelse(Subject_binary %in% c("Juveniles_Pooled", "Juvenile", "No_Birds", "Bird_of_Unknown_Age"), "No Adult", Subject_binary)
  )

# Looks good, only 2 values remain as expected 
unique(manual_scoring_nm2$Subject_binary)


# Get one row of PIT tag IDs per video recording event
glimpse(auto_scoring2)

# Create a new column that condenses PIT tag IDs from inferences about which individual started a movement event, which individual ended the movement event, and which individual was detected in a perching event
auto_scoring3 <- auto_scoring2 %>% 
  dplyr::mutate(
    PIT_tags = paste(individual_initiated, individual_ended, perching_PIT_tag, sep = " - ")
  )

glimpse(auto_scoring3)
unique(auto_scoring3$PIT_tags)

# Convert the subject labels in the ABISSMAL scored dataset to binary labels: Adult or No Adult
auto_scoring4 <- auto_scoring3 %>% 
  # Replace the PIT tag labels to be "Adult" or "No Adult"
  dplyr::mutate(
    Subject_binary = ifelse(grepl(paste(c("01-10-3F-8F-F0", "01-10-16-CD-1C"), collapse = "|"), PIT_tags), "Adult", PIT_tags),
    Subject_binary = ifelse(!grepl(paste(c("01-10-3F-8F-F0", "01-10-16-CD-1C"), collapse = "|"), PIT_tags), "No Adult", Subject_binary)
  )

# Looks good, only the two expected labels remain
unique(auto_scoring4$Subject_binary)

########### Convert the manual and automated scoring data frames to tables

# Get one row of subject labels per video recording event
manual_table <- manual_scoring_nm2 %>% 
  group_by(unmasked_video_file) %>%
  # Create a unique row identifier within videos
  dplyr::mutate(
    row_id = row_number()
  ) %>% 
  ungroup() %>% 
  # View()
  dplyr::select(row_id, unmasked_video_file, Subject_binary) %>% 
  pivot_wider(names_from = unmasked_video_file, values_from = Subject_binary) %>% 
  t()


auto_table <- auto_scoring4 %>% 
  group_by(assigned_recording_event) %>%
  # Create a unique row identifier within videos
  dplyr::mutate(
    row_id = row_number()
  ) %>% 
  ungroup() %>% 
  # View()
  dplyr::select(row_id, assigned_recording_event, Subject_binary) %>% 
  pivot_wider(names_from = assigned_recording_event, values_from = Subject_binary) %>% 
  t()

View(auto_table)
View(manual_table)

```

In this analysis, we need to remove video recording events that only included activity scored inside of the container, for a fair validation of the RFID system. The RFID antennae is mounted in the container entrance, so it can only detect activity by individuals at this location. We included videos that had activity scored for birds of an unknown age, but did not consider this subject label an adult detection to be conservative.
```{r}

# Iterate over all the unique videos included in the automated and manual scoring tables to find true positives and false negatives
# Remove video recording events that had INSC scores only
# unique(manual_scoring_nm$Behavior)

unique_videos <- manual_scoring_nm %>%
  # This filter will drop any rows with the INSC label
  # But since there are multiple rows per unique recording event, video recording events that had other labels will be retained in the unique() call below
  dplyr::filter(!grepl("INSC", Behavior)) %>% 
  pull(unmasked_video_file) %>% 
  unique()

unique_videos

glimpse(auto_table)
glimpse(manual_table)

# testing
# i <- 10

tp_fp_tn_fn <- data.table::rbindlist(pblapply(1:length(unique_videos), function(i){
  
  # Get unique subject labels for the given unique recording event from summarized tables of both scoring methods
  auto_scores <- unique(auto_table[grep(unique_videos[i], dimnames(auto_table)[[1]]), ])
  auto_scores <- auto_scores[!is.na(auto_scores)]
  
  manual_scores <- unique(manual_table[grep(unique_videos[i], dimnames(manual_table)[[1]]), ])
  manual_scores <- manual_scores[!is.na(manual_scores)]
  
  auto_scores
  manual_scores
  
  # Convert scores to Boolean using the adult activity label
  auto_a <- ifelse(any("Adult" %in% auto_scores), TRUE, FALSE)
  manual_a <- ifelse(any("Adult" %in% manual_scores), TRUE, FALSE)
  
  # Convert scores to Boolean using the no adult activity label
  auto_n <- ifelse(any("No Adult" %in% auto_scores), TRUE, FALSE)
  manual_n <- ifelse(any("No Adult" %in% manual_scores), TRUE, FALSE)
  
  # Score true positives: both methods identitfied a type of activity
  if(auto_a & manual_a){
    
    true_positive <- 1
    
  } else {
    
    true_positive <- 0
    
  }
  
  # Score false positives: manual scoring identified no adult activity but ABISSMAL did
  if(manual_n & auto_a){
    
    false_positive <- 1
    
  } else {
    
    false_positive <- 0
    
  }
  
  # Score true negatives: both methods identified no adult activity
  if(auto_n & manual_n){
    
    true_negative <- 1
    
  } else {
    
    true_negative <- 0
    
  }
  
  # Score false negatives: manual scoring identified adult activity but ABISSMAL did not
  if(manual_a & auto_n){
    
    false_negative <- 1
    
  } else {
    
    false_negative <- 0
    
  }
  
  return(
    data.frame(
      unique_video_recording_event = unique_videos[i],
      true_positive = true_positive,
      false_positive = false_positive,
      true_negative = true_negative,
      false_negative = false_negative
    )
  )
  
}))


# There should be 1 row per unique video recording event retained after filtering to remove videos scored as INSC only, looks good
glimpse(tp_fp_tn_fn)
# View(tp_fp_tn_fn)

```

Calculate precision, recall, sensitivity, and specificity of the RFID module, which ABISSMAL uses to detect activity by each adult individual.
```{r}

tp_fp_tn_fn_sum <- tp_fp_tn_fn %>% 
  dplyr::summarise(
    tps = sum(true_positive),
    fps = sum(false_positive),
    tns = sum(true_negative),
    fns = sum(false_negative)
  )

tp_fp_tn_fn_sum

data.frame(
  precision = precision(X = tp_fp_tn_fn_sum, tps_col_nm = "tps", fps_col_nm = "fps"),
  sensitivity = sensitivity(X = tp_fp_tn_fn_sum, tps_col_nm = "tps", fns_col_nm = "fns"),
  specificity = specificity(X = tp_fp_tn_fn_sum, tns_col_nm = "tps", fps_col_nm = "fns")
)

```


5. INDIVIDUAL IDENTITY DETECTION: Next, ask how reliably we captured individual identity with RFID compared to the manually scored videos. In this analysis, we will rely on the RFID data for the automated detection of individuals, which will have data for the PIT-tagged adults only. Focus this analysis on asking how often each PIT tag was detected for each adult compared to the manual scoring, and then both PIT tags when manual scoring identified activity by both adults (three overall levels of individual identity detection performance).

General approach to calculating individual identity detection performance:

**True positive** = The same individual(s) was detected by both ABISSMAL and manual scoring.

**False negative** = ABISSMAL detected the incorrect PIT tag compared to the individual identity detected by manual scoring. 

**False positive** = ABISSMAL detected the incorrect PIT tag compared to the individual identity detected by manual scoring.

**True negative** = Neither ABISSMAL nor manual scoring detected activity by an adult.

Overall, we can assess both the precision (out of the positive predictions, which are truly positive) and sensitivity (or recall, what percentage of the total positives were predicted) of activity location inferences by ABISSMAL, as well as the the specificity (true negative rate, or how well the true negatives are predicted while avoiding false alarms). Note that false negatives and positives will be the same because they are defined as a mismatch in location scores between methods.

Assess the precision, sensitivity, and specificity of individual identity detection via the RFID module used by ABISSMAL.

First we need to condense the automated scoring and the manual scoring data frames to have a single row per video while retaining information about unique adult identities (from PIT tags in the ABISSMAL data, and adult subject labels in the manually scored data).
```{r}

# View(manual_scoring_nm)
names(manual_scoring_nm)


########### Convert the subject labels per dataset

# The subject names for individually identified adults in the manually scored data are "Female", "Male".
# Subject names for no detections of adults or unidentifiable adults will be all other subject labels: "Juveniles_Pooled", "Juvenile", "No_Birds", "Bird_of_Unknown_Age" (to be conservative, since such subjects could have been juvenile), and "Unknown_Adult".

# The unique PIT tag IDs for adults in this dataset are: "01-10-3F-8F-F0" (Male) and "01-10-16-CD-1C" (Female). See the script for creating figures that was written previously.
# unique(scored_clusters$individual_initiated)

# Convert the subject labels in the manually scored dataset to 3 labels: Female, Male, or No Identifiable Adult
manual_scoring_nm2 <- manual_scoring_nm %>% 
  # Replace the subject labels as above
  dplyr::mutate(
    Subject_mod = ifelse(Subject %in% c("Juveniles_Pooled", "Juvenile", "No_Birds", "Bird_of_Unknown_Age", "Unknown_Adult"), "No Identifiable Adult", Subject)
  )

# Looks good, only 3 values remain as expected 
unique(manual_scoring_nm2$Subject_mod)


# Get one row of PIT tag IDs per video recording event
glimpse(auto_scoring2)

# Create a new column that condenses PIT tag IDs from inferences about which individual started a movement event, which individual ended the movement event, and which individual was detected in a perching event
auto_scoring3 <- auto_scoring2 %>% 
  dplyr::mutate(
    individual_initiated = gsub("01-10-3F-8F-F0", "Male", individual_initiated),
    individual_initiated = gsub("01-10-16-CD-1C", "Female", individual_initiated),
    individual_ended = gsub("01-10-3F-8F-F0", "Male", individual_ended),
    individual_ended = gsub("01-10-16-CD-1C", "Female", individual_ended),
    perching_PIT_tag = gsub("01-10-3F-8F-F0", "Male", perching_PIT_tag),
    perching_PIT_tag = gsub("01-10-16-CD-1C", "Female", perching_PIT_tag)
  ) %>% 
  dplyr::mutate(
    Adult_IDs = paste(individual_initiated, individual_ended, perching_PIT_tag, sep = " - ")
  )

glimpse(auto_scoring3)
unique(auto_scoring3$Adult_IDs)

# Convert the subject labels in the ABISSMAL scored dataset to binary labels: Adult or No Adult
auto_scoring4 <- auto_scoring3 %>% 
  # Replace the Adult_ID values to be similar to the manually scored data
  dplyr::mutate(
    Subject_mod = ifelse(grepl("NA - NA - NA", Adult_IDs), "No Identifiable Adult", Adult_IDs)
  )

# Looks good. These labels will include the same subject repeated or more than one subject
unique(auto_scoring4$Subject_mod)

########### Convert the manual and automated scoring data frames to tables

# Get one row of subject labels per video recording event
manual_table <- manual_scoring_nm2 %>% 
  group_by(unmasked_video_file) %>%
  # Create a unique row identifier within videos
  dplyr::mutate(
    row_id = row_number()
  ) %>% 
  ungroup() %>% 
  # View()
  dplyr::select(row_id, unmasked_video_file, Subject_mod) %>% 
  pivot_wider(names_from = unmasked_video_file, values_from = Subject_mod) %>% 
  t()


auto_table <- auto_scoring4 %>% 
  group_by(assigned_recording_event) %>%
  # Create a unique row identifier within videos
  dplyr::mutate(
    row_id = row_number()
  ) %>% 
  ungroup() %>% 
  # View()
  dplyr::select(row_id, assigned_recording_event, Subject_mod) %>% 
  pivot_wider(names_from = assigned_recording_event, values_from = Subject_mod) %>% 
  t()

View(auto_table)
View(manual_table)

```

In this analysis, we need to remove video recording events that only included activity scored inside of the container, for a fair validation of the RFID system. The RFID antennae is mounted in the container entrance, so it can only detect activity by individuals at this location. We included videos that had activity scored for birds of an unknown age, but did not consider this subject label an adult detection to be conservative.
```{r}

# Iterate over all the unique videos included in the automated and manual scoring tables to find true positives and false negatives
# Remove video recording events that had INSC scores only
# unique(manual_scoring_nm$Behavior)

unique_videos <- manual_scoring_nm %>%
  # This filter will drop any rows with the INSC label
  # But since there are multiple rows per unique recording event, video recording events that had other labels will be retained in the unique() call below
  dplyr::filter(!grepl("INSC", Behavior)) %>%
  pull(unmasked_video_file) %>% 
  unique()

unique_videos

glimpse(auto_table)
glimpse(manual_table)

# These labels correspond to the 3 levels of validation of individual identity detection: detecting of the adult female, the adult male, and both adults together
adults <- c("Female", "Male", "Both")
labels <- list(
  "Female" = "Female",
  "Male" = "Male",
  "Both" = c("Female", "Male")
)

labels

# testing
i <- 1
n <- 3

tp_fp_tn_fn <- data.table::rbindlist(pblapply(1:length(unique_videos), function(i){
  
  # Get unique behavioral scores for the given unique recording event from summarized tables of both scoring methods
  auto_scores <- unique(auto_table[grep(unique_videos[i], dimnames(auto_table)[[1]]), ])
  auto_scores <- auto_scores[!is.na(auto_scores)]
  
  manual_scores <- unique(manual_table[grep(unique_videos[i], dimnames(manual_table)[[1]]), ])
  manual_scores <- manual_scores[!is.na(manual_scores)]
  
  auto_scores
  manual_scores
  
  # Iterate over adult labels
  res <- data.table::rbindlist(lapply(1:length(adults), function(n){
    
    # Convert scores to Boolean using the labels associated with the respective location
    if(!adults[n] %in% "Both"){
      
      auto <- ifelse(any(grepl(labels[[adults[n]]], auto_scores)), TRUE, FALSE)
      manual <- ifelse(any(grepl(labels[[adults[n]]], manual_scores)), TRUE, FALSE)
      
      # auto
      # manual
      
      # If checking for labels in both locations, then loop over the location labels for a thorough Boolean conversion
    } else {
      
      tmp_labs <- labels[[adults[n]]]
      
      tmp_res <- data.table::rbindlist(lapply(1:length(tmp_labs), function(z){
        
        auto <- any(grepl(tmp_labs[z], auto_scores))
        manual <- any(grepl(tmp_labs[z], manual_scores))
        
        return(
          data.frame(
            auto = auto,
            manual = manual
          )
        )
        
      }))
      
      auto <- all(tmp_res$auto)
      manual <- all(tmp_res$manual)
      
    }
    
    # Score true positives
    if(auto & manual){
      
      true_positive <- 1
      
    } else {
      
      true_positive <- 0
      
    }
    
    # Score false positives
    if(auto & !manual){
      
      false_positive <- 1
      
    } else {
      
      false_positive <- 0
      
    }
    
    # Score true negatives
    if(!manual & !auto){
      
      true_negative <- 1
      
    } else {
      
      true_negative <- 0
      
    }
    
    # Score false negatives
    if(manual & !auto){
      
      false_negative <- 1
      
    } else {
      
      false_negative <- 0
      
    }
    
    return(
      data.frame(
        unique_video_recording_event = unique_videos[i],
        adult_labels = adults[n],
        true_positive = true_positive,
        false_positive = false_positive,
        true_negative = true_negative,
        false_negative = false_negative
      )
    )
    
  }))
  
  return(res)
  
}))

# There should be 3 rows per unique video recording event after the filtering above, looks good
length(unique_videos) * length(adults) == nrow(tp_fp_tn_fn)
glimpse(tp_fp_tn_fn)
# View(tp_fp_tn_fn)

```

Calculate precision, recall, sensitivity, and specificity of activity detection by adult.
```{r}

tp_fp_tn_fn_sum <- tp_fp_tn_fn %>% 
  group_by(adult_labels) %>% 
  dplyr::summarise(
    tps = sum(true_positive),
    fps = sum(false_positive),
    tns = sum(true_negative),
    fns = sum(false_negative)
  ) %>% 
  ungroup()

tp_fp_tn_fn_sum

# Precision across adults
precision_ind <- sapply(1:length(adults), function(i){
  
  precision(X = tp_fp_tn_fn_sum[grep(adults[i], tp_fp_tn_fn_sum$adult_labels), ], tps_col_nm = "tps", fps_col_nm = "fps")
  
})

# names(precision_ind) <- adults
precision_ind

# Sensitivity across adults
sensitivity_ind <- sapply(1:length(adults), function(i){
  
  sensitivity(X = tp_fp_tn_fn_sum[grep(adults[i], tp_fp_tn_fn_sum$adult_labels), ], tps_col_nm = "tps", fns_col_nm = "fns")
  
})

sensitivity_ind

# Specificity across adultss
specificity_ind <- sapply(1:length(adults), function(i){
  
  specificity(X = tp_fp_tn_fn_sum[grep(adults[i], tp_fp_tn_fn_sum$adult_labels), ], tns_col_nm = "tps", fps_col_nm = "fns")
  
})

specificity_ind

# Note that sensitivity and specificity are the same, given the way that false positives and negatives were defined

data.frame(
  adult_labels = adults,
  precision = precision_ind,
  sensitivity = sensitivity_ind,
  specificity = specificity_ind
)

```

6. BEHAVIORAL INFERENCE: Next, ask how well the integrated dataset of movement sensors scored the TYPE of activity detected. This is a finer-grained validation to ask how well behavioral scores associated with the video recording events match between the two methods. Check out my notes above that detail expectations - there will be more scores per video recording event using the manual method. The ABISSMAL functions also are currently designed to identify discrete behavioral events in a coarse-grained way, rather than longer sequences of these dicrete events. Therefore, the analyses below focus on asking how often each type of behavior was identified per unique video recording event per method, rather than comparing sequences of behavioral scores.

For the analysis below, we want to know how often ABISSMAL and manual scoring picked up the same behavior in association with a given video recording event. Here we will compare the 4 categories of coarse-grained behaviors identified by both methods: INSC, PERC, ENTR, EXIT. We will also focus on scoring of only a single behavior at a time here.

**True positive** = the same behavior was detected by both ABISSMAL and manual scoring.

**False negative** = ABISSMAL did not detect the same behavior that was detected by the manual scoring. We cannot detect a true lack of detections by ABISSMAL (e.g. no behavioral score at all), since this validation analysis relies on the same videos recorded by motion detection to compare scoring methods, and all of these video recordings are associated with an automated behavioral inference by default.

**False positive** = ABISSMAL detected a different behavior from the manual scoring, or the manual scoring did not detect any birds (NOBR).

**True negative** = neither ABISSMAL nor manual scoring detect the given behavior. We cannot calculate true negatives (e.g. no behavioral scores at all) for the same reasons as above with false negatives.

Overall, we can calculate the precision, sensitivity, and specificity of detecting types of behaviors. Note that the false negatives and false positives are not mutually exclusive, although here the false positive calculations will take the NOBR manual scoring into account (so the false positives and negatives will not be exactly the same).
```{r}

# First we need to condense the behavioral inferences from automated scoring into a single column, again
auto_scoring5 <- auto_scoring_extraAssignedVids %>%
  dplyr::mutate(
    perching_inference = ifelse(!is.na(perching_rfid_start) | !is.na(perching_outer_irbb_start) | !is.na(perching_inner_irbb_start), "perching", NA)) %>% 
  dplyr::mutate(
    inferences = paste(direction_scored, inferredMovement_Location, perching_inference, sep = "-")
  )

glimpse(auto_scoring5)
View(auto_scoring5)

table(auto_scoring5$inferences)

# no perching events represented here with this video subsample:

# entrance-container_entrance-NA      ### this is an entrance or ENTR
#                              6 
# exit-container_entrance-NA          ### this is an exit or EXIT
#                              3 
# NA-inside_container-NA              ### this is inside_container or INSC
#                             44 

# Will need to add back perching events for the full dataset (currently that inference is NA here)
auto_scoring5 <- auto_scoring5 %>% 
  dplyr::mutate(
    comb_behav_inf = ifelse(grepl("^entrance-|-entrance", inferences), "ENTR", inferences),
    comb_behav_inf = ifelse(grepl("^exit-|-exit", comb_behav_inf), "EXIT", comb_behav_inf),
    comb_behav_inf = ifelse(grepl("inside_container", comb_behav_inf), "INSC", comb_behav_inf),
    comb_behav_inf = ifelse(grepl("perch", comb_behav_inf), "PERC", comb_behav_inf)
  )

# Looks good
table(auto_scoring5$inferences)
table(auto_scoring5$comb_behav_inf)
glimpse(auto_scoring2)

```

Then we need to condense the automated scoring and the manual scoring data frame to have a single row per video.
```{r}

# View(manual_scoring_nm)
names(manual_scoring_nm)

# Get one row of behavioral labels per video recording event
manual_table <- manual_scoring_nm %>% 
  group_by(unmasked_video_file) %>%
  # Create a unique row identifier within videos
  dplyr::mutate(
    row_id = row_number()
  ) %>% 
  ungroup() %>% 
  # View()
  dplyr::select(row_id, unmasked_video_file, Behavior) %>% 
  pivot_wider(names_from = unmasked_video_file, values_from = Behavior) %>% 
  t()

auto_table <- auto_scoring5 %>% 
  group_by(assigned_recording_event) %>%
  # Create a unique row identifier within videos
  dplyr::mutate(
    row_id = row_number()
  ) %>% 
  ungroup() %>% 
  # View()
  dplyr::select(row_id, assigned_recording_event, comb_behav_inf) %>% 
  pivot_wider(names_from = assigned_recording_event, values_from = comb_behav_inf) %>% 
  t()

# View(auto_scoring2)

View(auto_table)
View(manual_table)

```

Calculate the true and false positives, and the true and false negatives.
```{r}

# Looks good
length(unique(auto_scoring5$assigned_recording_event))
length(unique(manual_scoring_nm$unmasked_video_file))

# Iterate over all the unique videos included in the automated and manual scoring tables
unique_videos <- unique(manual_scoring_nm$unmasked_video_file)
unique_videos

glimpse(auto_table)
glimpse(manual_table)

behavior_labels <- c("PERC", "ENTR", "EXIT", "INSC")
no_behavior_label <- "NOBR"

# i <- 2
# n <- 2

tp_fp_tn_fn <- data.table::rbindlist(pblapply(1:length(unique_videos), function(i){
  
  # Get unique behavioral scores for the given unique recording event from summarized tables of both scoring methods
  auto_scores <- unique(auto_table[grep(unique_videos[i], dimnames(auto_table)[[1]]), ])
  auto_scores <- auto_scores[!is.na(auto_scores)]
  
  manual_scores <- unique(manual_table[grep(unique_videos[i], dimnames(manual_table)[[1]]), ])
  manual_scores <- manual_scores[!is.na(manual_scores)]
  
  # auto_scores
  manual_scores
  
  # Iterate over the activity labels
  tmp_res <- data.table::rbindlist(lapply(1:length(behavior_labels), function(n){
    
    # Convert scores to Boolean using the activity labels
    auto_a <- ifelse(any(behavior_labels[n] %in% auto_scores), TRUE, FALSE)
    manual_a <- ifelse(any(behavior_labels[n] %in% manual_scores), TRUE, FALSE)
    
    # Convert scores to Boolean using the no activity label
    auto_n <- ifelse(any(no_behavior_label %in% auto_scores), TRUE, FALSE)
    manual_n <- ifelse(any(no_behavior_label %in% manual_scores), TRUE, FALSE)
    
    # auto_a
    # manual_a
    # auto_n
    # manual_n
    
    # Score true positives: both methods identitfied a type of activity
    if(auto_a & manual_a){
      
      true_positive <- 1
      
    } else {
      
      true_positive <- 0
      
    }
    
    # Score false positives: manual scoring identified NOBR or the given type of behavior but ABISSMAL detected a different behavior
    if((!manual_a & auto_a) | (manual_n & !auto_n)){
      
      false_positive <- 1
      
    } else {
      
      false_positive <- 0
      
    }
    
    # Score true negatives: both methods identitfied a different type of activity
    if(auto_n & manual_n){
      
      true_negative <- 1
      
    } else {
      
      true_negative <- 0
      
    }
    
    # Score false negatives: ABISSMAL detected the given behavior but manual scoring identified a different behavior
    if(manual_a & !auto_a){
      
      false_negative <- 1
      
    } else {
      
      false_negative <- 0
      
    }
    
    return(
      data.frame(
        unique_video_recording_event = unique_videos[i],
        behavior_label = behavior_labels[n],
        true_positive = true_positive,
        false_positive = false_positive,
        true_negative = true_negative,
        false_negative = false_negative
      )
    )
    
    
  }))
  
}))

# There should be 4 rows for each of the unique video recording events, looks good
glimpse(tp_fp_tn_fn)
length(unique_videos) * length(behavior_labels) == nrow(tp_fp_tn_fn)

```

Calculate precision, recall, sensitivity, and specificity of activity detection by behavioral activity.
```{r}

tp_fp_tn_fn_sum <- tp_fp_tn_fn %>% 
  group_by(behavior_label) %>% 
  dplyr::summarise(
    tps = sum(true_positive),
    fps = sum(false_positive),
    tns = sum(true_negative),
    fns = sum(false_negative)
  ) %>% 
  ungroup()

tp_fp_tn_fn_sum

# Precision across behaviors
precision_ind <- sapply(1:length(behavior_labels), function(i){
  
  precision(X = tp_fp_tn_fn_sum[grep(behavior_labels[i], tp_fp_tn_fn_sum$behavior_label), ], tps_col_nm = "tps", fps_col_nm = "fps")
  
})

precision_ind

# Sensitivity across behaviors
sensitivity_ind <- sapply(1:length(behavior_labels), function(i){
  
  sensitivity(X = tp_fp_tn_fn_sum[grep(behavior_labels[i], tp_fp_tn_fn_sum$behavior_label), ], tps_col_nm = "tps", fns_col_nm = "fns")
  
})

sensitivity_ind

# Specificity across behaviors
specificity_ind <- sapply(1:length(behavior_labels), function(i){
  
  specificity(X = tp_fp_tn_fn_sum[grep(behavior_labels[i], tp_fp_tn_fn_sum$behavior_label), ], tns_col_nm = "tps", fps_col_nm = "fns")
  
})

specificity_ind

# Note that sensitivity and specificity are not exactly the same here, given the way that false positives and negatives were defined

data.frame(
  behavior_label = behavior_labels,
  precision = precision_ind,
  sensitivity = sensitivity_ind,
  specificity = specificity_ind
)

```

8. TIMING AND DURATION OF BEHAVIORS.

For each behavioral event scored between methods, how well do the timestamps align? This analysis will depend on finding an "optimal" set of data processing and integration parameters for the automated scoring using the comparisons above to the manually scored baseline.  Alternatively, the analysis could be done in a way that facilitates comparisons across these parameters, but this could get messy.

What I want to know is: when a given behavioral event was identified by both methods, when did this event begin and end, and how do these timestamps compare between methods?

TKTK consider updating `score_clusters` to assign the end timestamp of INSC events as the end of the video recording event. Currently the end timestamp is the same as the start, and we need the true video recording event duration to do this (which is currently hard to access but in this analysis can be pulled from BORIS). This needs to be done in a later version of the ABISSMAL functions.
```{r}

# Iterate over all the unique videos included in the automated and manual scoring tables
unique_videos <- unique(manual_scoring_nm$unmasked_video_file)
unique_videos

glimpse(auto_table)
glimpse(manual_table)

behavior_labels <- c("PERC", "ENTR", "EXIT", "INSC")

# i <- 2
# n <- 4

temporal_validtn <- data.table::rbindlist(pblapply(1:length(unique_videos), function(i){
  
  # Get unique behavioral scores for the given unique recording event from summarized tables of both scoring methods
  auto_scores <- unique(auto_table[grep(unique_videos[i], dimnames(auto_table)[[1]]), ])
  auto_scores <- auto_scores[!is.na(auto_scores)]
  
  manual_scores <- unique(manual_table[grep(unique_videos[i], dimnames(manual_table)[[1]]), ])
  manual_scores <- manual_scores[!is.na(manual_scores)]
  
  # auto_scores
  # manual_scores
  
  # Iterate over the activity labels to determine whether the same behavior was scored by both methods within this video recording event time slot
  tmp_res <- data.table::rbindlist(lapply(1:length(behavior_labels), function(n){
    
    # Convert scores to Boolean using the activity labels
    auto_a <- ifelse(any(behavior_labels[n] %in% auto_scores), TRUE, FALSE)
    manual_a <- ifelse(any(behavior_labels[n] %in% manual_scores), TRUE, FALSE)
    
    # If the given behavior was scored by both methods, then find the start and the end timestamps of the behavioral state identified by both methods
    if(auto_a & manual_a){
      
      # Account for multiple behavioral events of the given type in each dataset for the given video, this may be possible (especially for the manually scored dataset). Currently, this analysis will only compare the very first exemplar of the given behavior when multiple are present for the given video recording event (for instance, when two subjects were scored as being inside of the container for the entire duration of the video recording event)
      
      # Search for temporal coordinates in the automatically scored dataset
      tmp_auto <- auto_scoring5 %>% 
        # First search for the unique recording event in this iteration
        dplyr::filter(grepl(unique_videos[i], assigned_recording_event)) %>% 
        # Then search for the behavioral label that matches the current iteration
        dplyr::filter(comb_behav_inf == behavior_labels[n]) %>% 
        slice(1)
      
      # Search for temporal coordinates in the manually scored dataset
      tmp_manual <- manual_scoring_nm %>% 
        # First search for the unique recording event in this iteration
        dplyr::filter(grepl(unique_videos[i], unmasked_video_file)) %>% 
        # Then search for the behavioral label that matches the current iteration
        dplyr::filter(Behavior == behavior_labels[n]) %>% 
        slice(1)
      
      # If the current behavior was INSC, then update the end timestamp automatically assigned by ABISSMAL to instead be the end of the unique recording event
      # Pull the duration of the given recording event from the manually scored dataset (BORIS returns this information automatically)
      if(behavior_labels[n] == "INSC"){
        
        # Get the complete recording event duration in seconds
        recording_event_dur <- sum(as.numeric(strsplit(tmp_manual$Media.duration..s., split = ";")[[1]]))
        
        # Add the recording event duration to the automated start timestamp
        tmp_auto$end <- tmp_auto$start + recording_event_dur
        
      }
      
      # Get a POSIX format timestamp for the manually scored videos using the video recording file name (which indicates the timestamp at which video recording started)
      timestamp <- gsub("Box_03_|pre-trigger.mp4|pre_trigger.mp4", "", unique_videos[i])
      timestamp <- strsplit(timestamp, split = "_")[[1]]
      timestamp <- paste(paste(timestamp[1:3], collapse = "-"), paste(timestamp[4:length(timestamp)], collapse = ":"), sep = " ")
      timestamp <- as.POSIXct(format(as.POSIXct(timestamp, tz = ""), "%Y-%m-%d %H:%M:%OS6"))
      timestamp
      
      # Then add the start and the end timestamps in seconds from BORIS to this overall timestamp to get complete start and end timestamps that can be compared to the automated data
      manual_start <- timestamp + tmp_manual$`Start..s.`
      manual_end <- timestamp + tmp_manual$`Stop..s.`
      
      return(
        data.frame(
          unique_video_recording_event = unique_videos[i],
          behavior_label = behavior_labels[n],
          auto_start = tmp_auto %>% 
            pull(start),
          auto_end = tmp_auto %>% 
            pull(end),
          manual_start = manual_start,
          manual_end = manual_end
        ) %>% 
          dplyr::mutate(
            auto_duration = auto_end - auto_start,
            manual_duration = manual_end - manual_start
          )
      )
      
    }
    
  }))
  
}))


glimpse(temporal_validtn)
View(temporal_validtn)

temporal_validtn %>% 
  dplyr::mutate(
    start_diff = manual_start - auto_start,
    end_diff = manual_end - auto_end,
    dur_diff = manual_duration - auto_duration
  ) %>% 
  dplyr::summarise(
    mean_start_diff = mean(start_diff),
    mean_end_diff = mean(end_diff),
    mean_dur_diff = mean(dur_diff)
  )

```

TKTK continue here

8. Automated data processing: Iterate over parameters that we use for the automated scoring, especially the temporal thresholds used for clustering, to create different automated scoring datasets for the analyses below

Once the pipeline above is developed, then run the full validation analysis pipeline with all 4045 video recording events
