---
title: "Video validation analysis"
author: "Grace Smith-Vidaurre"
date: "2024-12-01"
output: html_document
---

Purpose: Perform a validation analysis of automatically scored behaviors (e.g. behavioral inferences) obtained by ABISSMAL. The computational analyses available in ABISSMAL facilitate integrating data across movement sensors to derive coarse-grained behavioral inferences. Here, we will validate those behavioral inferences for a subsample of the full set of videos used in the methods manuscript, using a baseline dataset obtained by manual video scoring to obtain coarse-grained behavioral states for adult birds and juveniles. The current plan for this validation analysis includes 1) assessing whether the automated and manual scoring return similar levels of overall activity per unique recording event, 2) whether the automated and manual scoring return similar levels of overall activity per behavioral state in each unique recording event, 3) the degree to which the timestamps for these automated and manual scores agree with one another (this will be a bit tricky given that some of the movement sensors will detect movement before the videos), 4) we can also iterate over parameter combinations for the automated scoring to ask how these decisions influence ageement with the manual dataset.

The manually scored data needs to be concatenated and checked prior to running the final validation analysis. Also, Summer noticed some video recording events had videos switched in order, so we have to pre-process the manually scored dataset to fix these issues for the videos she caught that had this issue.

Finally, the validation analysis will be planned out and run with a random subsample of the videos used for this analysis.

```{r}

knitr::opts_knit$set(eval = TRUE, echo = TRUE)

```

```{r package and paths, warning = FALSE, message = FALSE}

# Clean the global environment
rm(list = ls())

X <- c("tidyverse", "pbapply", "data.table", "tidyquant")

invisible(lapply(X, library, character.only = TRUE))

# Load the custom functions for computational analyses with ABISSMAL
code_path <- "~/Desktop/GitHub_repos/ABISSMAL/R"
code <- list.files(code_path, pattern = ".R$", full.names = TRUE)

invisible(lapply(1:length(code), function(i){
  source(code[i])
}))

# Path to the aggregated BORIS spreadsheet in wide format across all video recording events
path <- "~/Desktop/ABISSMAL_validation_data"

# Path to the combined raw movement sensor data
sensor_path <- file.path(path, "raw_combined")

# Initialize a folder where we will save the indexed raw data for the validation analyses
indexed_dir <- "indexed_raw_data"
indexed_path <- file.path(path, indexed_dir)

# Create this folder if it doesn't already exist
if(!dir.exists(indexed_path)){
  dir.create(indexed_path)
}

seed <- 888

```

Read in the aggregated manual scoring spreadsheet in which each row is a unique behavioral state per subject within a recording event.
```{r}

manual_scoring <- read.csv(file.path(path, "BORIS_ManualScoring_Aggregated_pre-processed.csv"))
glimpse(manual_scoring)

```

Read in the spreadsheet for unmasking the video file names.
```{r}

unmask_nms <- read.csv(file.path(path, "unmasked_video_names.csv"))
glimpse(unmask_nms)

```

Next steps:

1a. Data pre-processing of the switched recording events that Summer identified. Look at the spreadsheet called "BORIS_VideoList_TimeFlipped.csv". We could i) exclude these videos from analyses, or ii) we could try to flip the manual scores given the timestamps of the scores and the videos themselves, or iii) if one video has extremely short duration, then drop the scores for that video in the pair. TKTK keep thinking about a solution

1b. Figure out if we can discriminate videos for which video recording was initialized by daily animal checks (a hand opening the nest container), check the notes column of the aggregated .csv. This will be scored as no birds inside of the containr but may have a very high pixel number that changed. Summer kept track in her log, but this only happened 2-3 times, check the notes of the BORIS .csv, recording event 2322 

2. We need to choose a subsample of these videos to develop the validation analysis pipeline.

```{r}

# Randomly select 50 video recording events from the current version of the aggregated manual scoring results in wide format

# First get all of the unique BORIS Observation IDs that correspond to unique recording events
unique_obs_ids <- unique(manual_scoring$Observation.id)
head(unique_obs_ids)

# There should be 4045 Observation IDs to match the unique recording events that Summer scored, looks good
length(unique_obs_ids)

set.seed(seed)
subsample <- sample(unique_obs_ids, 50, replace = FALSE)

subsample

```

3. Start writing out the steps of the validation analysis, run these with the subsample of videos chosen above

Pseudocode for the analysis:

i) Data Processing:

- Get the manual scoring data from BORIS in aggregated format (done)

- Decide how to handle the time-flipped videos (in progress)

- Unmask the video file names used for manual scoring

```{r}

manual_scoring_ss <- manual_scoring %>% 
  dplyr::filter(Observation.id %in% subsample)

dim(manual_scoring_ss)

# Checking filtering, looks good
# all(unique(manual_scoring_ss$Observation.id) %in% subsample)
# all(subsample %in% unique(manual_scoring_ss$Observation.id))

# Add the unmasked video file names to each observation in the manual scoring spreadsheet
# i <- 1 # testing
manual_scoring_nm <- data.table::rbindlist(pblapply(1:nrow(manual_scoring_ss), function(i){
  
  # Get the masked video recording event ID
  masked_nm <- manual_scoring_ss$masked_video_recording_event_ID[i]
  # masked_nm
  
  # Use the masked video recording event ID to find the unmasked recording event ID per row
  unmasked_nm <- unmask_nms %>% 
    dplyr::filter(masked_name == masked_nm) %>% 
    pull(videos)
  
  # Add the unmasked name back to the manual scoring data frame
  # The unmasked name will be one of the two unique video files per unique recording event (we don't need both names for later steps)
  return(
    manual_scoring_ss[i, ] %>% 
      dplyr::mutate(
        unmasked_video_file = unmasked_nm
      )
  )
  
}))

# Select a few rows at random and manually check the unmasking results
glimpse(manual_scoring_nm)
dim(manual_scoring_nm)
View(manual_scoring_nm)

```

Get the raw data across movement sensors used for analyses in the current methods manuscript.
```{r}

# The movement sensor data that we are using here is the raw data that we have used in the methods manuscript, and it's already been combined across days of data collection using ABISSMAL functions. We have one spreadsheet per sensor type, and the beam breaker spreadsheet contains data for 2 pairs of beam breakers
list.files(sensor_path)

# We will use this data to index the raw movement sensor data by the timestamps of the videos that Summer scored. The spreadsheet for the video recording events contains timestamp columns that correspond to the time at which the video camera detected motion and began recording the post-movement video (this timestamp is also in the filenames of each of the videos per unique recording event).
video_events <- read.csv(file.path(sensor_path, "combined_raw_data_Video.csv")) %>% # Make sure that the timestamps are in the right format
  dplyr::mutate(
    timestamp_ms = as.POSIXct(format(as.POSIXct(timestamp_ms, tz = ""), "%Y-%m-%d %H:%M:%OS6")))

glimpse(video_events)

# View(video_events)

# In the BORIS manual scoring data, we can use the column of Media File Duration to extract the exact duration of each video in each unique recording event (lots of variability in the duration of the pre-movement video recorded using the ring buffer)

```

Use the unmasked video file names to get the start timestamp of the post-movement video in each unique recording event in the spreadsheet of video recording events recorded by ABISSMAL. In this metadata spreadsheet for the video recordings he column timestamp_ms that holds the time when the camera detected motion and started recording video. Use these timestamps along with the durations of the pre- and post-movement videos from the aggregated BORIS manual scoring spreadsheet to obtain the start and end timestamps of each unique recording event (the pre- and post-movement videos together).
```{r}

# Get the unique prefixes for unmasked video recording event names
unique_video_pats <- manual_scoring_nm %>% 
  pull(unmasked_video_file) %>% 
  gsub("pre_trigger.mp4|_post_trigger.mp4", "", .) %>% 
  unique()

unique_video_pats

# Filter the video recording event metdata spreadsheet to obtain the start timestamps for the post-movement video per unique recording event
# i <- 1
recording_event_ts <- data.table::rbindlist(pblapply(1:length(unique_video_pats), function(i){
  
  # Get the start timestamp for the post-movement video of the current unique recording event
  start_post <- video_events %>% 
    dplyr::filter(grepl(unique_video_pats[i], video_file_name)) %>% 
    pull(timestamp_ms) %>% 
    unique()
  
  # Get the duration of each video file in the given recording event
  tmp <- manual_scoring_nm %>% 
    dplyr::filter(grepl(unique_video_pats[i], unmasked_video_file)) %>% 
    separate(
      ., col = "Media.duration..s.", into = c("video_1_dur", "video_2_dur"), sep = ";", remove = FALSE
    ) %>% 
    dplyr::select(c("video_1_dur", "video_2_dur")) %>% 
    distinct() %>%
    # Convert the duration in seconds to numeric
    dplyr::mutate(
      video_1_dur = as.numeric(video_1_dur),
      video_2_dur = as.numeric(video_2_dur)
    )
  
  # Create the start timestamp for this unique recording event by subtracting the duration of the first video in this event from the timestamp of the post-motion video
  start_event <- start_post - tmp$video_1_dur
  # start_event
  
  # Create the end timestamp for this unique recording event by adding the duration of the second video to the start timestamp of the post-motion video
  end_event <- start_post + tmp$video_2_dur
  # end_event
  
  # Check that the duration of these timestamps yields the same duration as the sum of the media durations in the BORIS spreadsheet
  offset <- as.numeric(end_event - start_event) - (tmp$video_1_dur + tmp$video_2_dur)
  
  if(offset < 0.01){
    
    return(
      data.frame(
        unique_recording_event_ID = unique_video_pats[i],
        start_event = start_event,
        end_event = end_event
      )
    )
    
  } else {
    
    stop("The video durations do not line up")
    
  }
  
}))

# We can use the timestamps to index the raw detections for all other movement sensors for validation
glimpse(recording_event_ts)
# View(recording_event_ts)

```

Read in the combined raw data for each sensor type.
```{r}

# Read in the raw combined infrared beam breaker detections
irbb_events <- read.csv(file.path(sensor_path, "combined_raw_data_IRBB.csv")) %>% # Make sure that the timestamps are in the right format
  dplyr::mutate(
    timestamp_ms = as.POSIXct(format(as.POSIXct(timestamp_ms, tz = ""), "%Y-%m-%d %H:%M:%OS6")))

glimpse(irbb_events)

# Read in the raw combined RFID detections
rfid_events <- read.csv(file.path(sensor_path, "combined_raw_data_RFID.csv")) %>% # Make sure that the timestamps are in the right format
  dplyr::mutate(
    timestamp_ms = as.POSIXct(format(as.POSIXct(timestamp_ms, tz = ""), "%Y-%m-%d %H:%M:%OS6")))

glimpse(rfid_events)

# Read in the raw combined video detections
video_events <- read.csv(file.path(sensor_path, "combined_raw_data_Video.csv")) %>% # Make sure that the timestamps are in the right format
  dplyr::mutate(
    timestamp_ms = as.POSIXct(format(as.POSIXct(timestamp_ms, tz = ""), "%Y-%m-%d %H:%M:%OS6")))

glimpse(video_events)

```

Index the raw data per movement sensor type by the start and end timestamps of all of the videos used for manual scoring for validation.
```{r}

# Iterate over rows in the video recording event timestamps to pull out raw data from each of the movement sensor types (including the videos themselves), using a buffer of 2 seconds before and after the start and end timestamps of each unique video recording event

# Initialize the movement sensor types (one per raw data spreadsheet)
sensor_type <- c("IRBB", "RFID", "Video")

# Initialize a list of the raw data objects
raw_data <- list(
  irbb_events,
  rfid_events,
  video_events
)

names(raw_data) <- sensor_type
str(raw_data)

# str(raw_data[["Video"]])

# Initialize the temporal buffer for indexing (seconds)
buf <- 2

# Testing
# i <- 1
# j <- 10

invisible(pblapply(1:length(sensor_type), function(i){
  
  res <- data.table::rbindlist(lapply(1:nrow(recording_event_ts), function(j){
    
    tmp <- raw_data[[sensor_type[i]]] %>% 
      dplyr::filter(
        timestamp_ms >= recording_event_ts$start_event[j] - buf & 
          timestamp_ms <= recording_event_ts$end_event[j] + buf
      )
    
    return(tmp)
    
  }))
  
  # The Video has the exact expected number of rows (100) for 50 unique recording events
  res %>% 
    write.csv(file.path(indexed_path, paste(paste("combined_raw_data", sensor_type[i], sep = "_"), ".csv", sep = "")), row.names = FALSE)
  
}))

```

Looks good. The next step will be to perform automated behavioral scoring of the indexed raw movement sensor data using the default temporal thresholds for automated data processing and behavioral inference from ABISSMAL computational analyses.

Find RFID and beam breaker perching events with indexed movement data....TKTK keep thinking about perching events and change the code below.
```{r TKTK find perching events}

detect_perching_events(file_nm = "combined_raw_data_RFID.csv", threshold = 2, run_length = 2, sensor_id_col_nm = "sensor_id", timestamps_col_nm = "timestamp_ms", PIT_tag_col_nm = "PIT_tag_ID", rfid_label = "RFID", outer_irbb_label = NULL, inner_irbb_label = NULL, general_metadata_cols = c("chamber_id", "sensor_id"), path = data_path, data_dir = "raw_combined", out_dir = "processed", out_file_prefix = "perching_events", tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")

detect_perching_events(file_nm = "combined_raw_data_IRBB.csv", threshold = 2, run_length = 2, sensor_id_col_nm = "sensor_id", timestamps_col_nm = "timestamp_ms", PIT_tag_col_nm = NULL, rfid_label = NULL, outer_irbb_label = "Outer Beam Breaker", inner_irbb_label = "Inner Beam Breaker", general_metadata_cols = c("chamber_id", "sensor_id"), path = data_path, data_dir = "raw_combined", out_dir = "processed", out_file_prefix = "perching_events", tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")

```

# Pre-processing

Pre-process the RFID, beam breaker, and video data for the indexed detections per sensor.
```{r preprocess data}

# Used thinning with a threshold of 2 seconds for beam breakers as well
preprocess_detections(sensor = "IRBB", timestamps_col_nm = "timestamp_ms", group_col_nm = "sensor_id", pixel_col_nm = NULL, mode = "thin", thin_threshold = 2, pixel_threshold = NULL, drop_tag = NULL, path = path, data_dir = "indexed_raw_data", out_dir = "indexed_processed", tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")

# Performed pre-processing with a threshold of 2 seconds for RFID because I removed the timestamp difference rounding
# The PIT tag IDs specified in `drop_tag` were tags I used for testing the RFID antenna and were not used to mark birds
preprocess_detections(sensor = "RFID", timestamps_col_nm = "timestamp_ms", group_col_nm = "PIT_tag_ID", pixel_col_nm = NULL, mode = "thin", thin_threshold = 2, pixel_threshold = NULL, drop_tag = c("01-10-3F-84-FC", "01-10-16-B8-7F"), path = path, data_dir = "indexed_raw_data", out_dir = "indexed_processed", tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")

# Used a pixel threshold of 1000 for video data. Since we used 9000 pixels as the sensitivity threshold for motion detection, this filtering threshold will not drop any data
# This filtering should not drop any videos in the indexed data, and looks good, 100 rows remain
preprocess_detections(sensor = "Video", timestamps_col_nm = "timestamp_ms", group_col_nm = NULL, pixel_col_nm = "total_pixels_motionTrigger", mode = NULL, thin_threshold = NULL, pixel_threshold = 1000, drop_tag = NULL, path = path, data_dir = "indexed_raw_data", out_dir = "indexed_processed", tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")

```

# Integration

Find clusters of detections across all 3 movement sensor that represent movement events per.
```{r find detection clusters for 3 sensor types}

# Using camera_label = "Camera" again here since that's was carried through from the raw data
# The run length needs to be set to 1 in order to correctly detect detection clusters of length 2
detect_clusters(file_nms = c("pre_processed_data_IRBB.csv", "pre_processed_data_RFID.csv", "pre_processed_data_Video.csv"), threshold = 2, run_length = 1, sensor_id_col_nm = "sensor_id", timestamps_col_nm = "timestamp_ms", PIT_tag_col_nm = "PIT_tag_ID", rfid_label = "RFID", camera_label = "Camera", preproc_metadata_col_nms = c("thin_threshold_s", "pixel_threshold", "data_stage", "date_pre_processed"), general_metadata_col_nms = c("chamber_id", "year", "month", "day"), video_metadata_col_nms = c("total_pixels_motionTrigger", "video_file_name"), path = path, data_dir = "indexed_processed", out_dir = "indexed_processed", out_file_nm = "detection_clusters.csv", tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")

```

Score detection clusters to make inferences about behavioral events associated with these movements. TKTK need to later integrate perching events (perching_dataset = "RFID-IRBB", perching_prefix = "perching_events_"), also sensor_id_col_nm, PIT_tag_col_nm cannot be NULL
```{r score detection clusters for 3 sensor types}

score_clusters(file_nm = "detection_clusters.csv", rfid_label = "RFID", camera_label = "Camera", outer_irbb_label = "Outer Beam Breaker", inner_irbb_label = "Inner Beam Breaker", video_metadata_col_nms = c("total_pixels_motionTrigger", "video_file_name"), integrate_perching = FALSE, sensor_id_col_nm = NULL, PIT_tag_col_nm = NULL, pixel_col_nm = "total_pixels_motionTrigger", video_width = 1280, video_height = 720, integrate_preproc_video = TRUE, video_file_nm = "pre_processed_data_Video.csv", timestamps_col_nm = "timestamp_ms", path = path, data_dir = "indexed_processed", out_dir = "indexed_processed", out_file_nm = "scored_detectionClusters.csv", tz = "", POSIXct_format = "%Y-%m-%d %H:%M:%OS")

```

Next, we need to check that the automated and manual datasets are in similar format in order to make a 1:1 plot for the very first validation figure.
```{r}

auto_scoring <- read.csv(file.path(path, "indexed_processed", "scored_detectionClusters.csv")) %>% 
  # Make sure that the timestamps are in the right format
  dplyr::mutate(
    start = as.POSIXct(format(as.POSIXct(start, tz = ""), "%Y-%m-%d %H:%M:%OS6")),
    end = as.POSIXct(format(as.POSIXct(end, tz = ""), "%Y-%m-%d %H:%M:%OS6"))
  )

# Check out the automated inferences and the manual scoring results (filtered by the subsample of videos used to develop this code, and unmasked)
glimpse(auto_scoring)
glimpse(manual_scoring_nm)

# View(auto_scoring)
# View(manual_scoring_nm)

# These columns contain the name of the video file associated with each event that was scored. The automated inferences will have NAs in this column when a video recording event was not associated with the given event
auto_scoring$video_file_name
manual_scoring_nm$unmasked_video_file

# The manual scoring protocol was oriented to score several behavioral states (and per subject) across each video recording event, while the automated scoring can identify multiple behavioral events associated with each video recording event, but is less likely to identify as many as the manual scoring given that we have more limited individual resolution (RFID at the container entrance only, and this can fail to detect PIT tags). In the automated inferences, are there cases when a single video recording event was used for more than one behavioral score?

# Yes, although in this subsample of the data there is only 1 video recording event that meets this condition
length(auto_scoring$video_file_name[!is.na(auto_scoring$video_file_name)])
length(unique(auto_scoring$video_file_name[!is.na(auto_scoring$video_file_name)]))
which(duplicated(auto_scoring$video_file_name[!is.na(auto_scoring$video_file_name)]))
auto_scoring$video_file_name[!is.na(auto_scoring$video_file_name)][17]

auto_scoring %>% 
  dplyr::filter(video_file_name == "Box_03_2023_7_22_13_24_42_pre_trigger.mp4") %>% 
  View()

# How many automated scores have no associated video?
# Two rows
auto_scoring %>% 
  dplyr::filter(is.na(video_file_name)) %>% 
  nrow()


# Iterate over rows in the automated scores, and for scored events not associated with videos, figure out whether those timestamps align with the start and end of a unique recording event with the added time buffer on either end
auto_scoring_extraAssignedVids <- data.table::rbindlist(lapply(1:nrow(auto_scoring), function(i){
  
  cat(paste("i = ", i, "\n", sep = " "))
  
  tmp_auto <- auto_scoring %>% 
    slice(i)
  
  if(is.na(tmp_auto$video_file_name)){
    
    # This filter will only find matches when the non-video sensor event started before the video recording
    tmp <- recording_event_ts %>% 
      # Filter using the start time, and bound the end timestamp to fall within 18 seconds after the start (to account for recording events composed of a video of ~6 seconds, then a video of ~ 10 seconds, sometimes up to 18 seconds together)
      dplyr::filter(
        start_event >= tmp_auto$start & end_event <= (tmp_auto$start + 18)
      )
    
    # Perform a search for events that occurred in the middle of a video recording if the search above didn't return results
    if(nrow(tmp) == 0){
      
      tmp <- recording_event_ts %>%
        # This filter will only find matches when the non-video sensor event started after the video recording
        dplyr::filter(
          tmp_auto$start >= start_event & tmp_auto$start <= end_event
        )
      
    }
    
    if(nrow(tmp) > 0){
      
      # Return the current row with the video recording event assigned as above
      res <- tmp_auto %>% 
        dplyr::mutate(
          assigned_recording_event = tmp$unique_recording_event_ID
        )
      
    } else {
      
      # Return the current row with the video recording event assigned as missing
      res <- tmp_auto %>% 
        dplyr::mutate(
          assigned_recording_event = NA
        )
      
    }
    
  } else {
    
    res <- tmp_auto %>% 
      dplyr::mutate(
        assigned_recording_event = video_file_name
      )
    
  }
  
  return(res)
  
}))


glimpse(auto_scoring_extraAssignedVids)

# All values that are missing the suffix "pre-trigger" or "post-trigger" were assigned in the loop above
unique(auto_scoring_extraAssignedVids$assigned_recording_event)

# Then we need to summarize the automated and manually scored data to yield a single row per video recording event, in order to make a 1:1 plot. In this plot, there should be more behavioral states scored manually than using the automated inferences (see my rationale above)
auto_scoring_summ <- auto_scoring_extraAssignedVids %>%
  # Drop rows that did not have an associated video recording event
  dplyr::filter(!is.na(video_file_name)) %>% 
  group_by(video_file_name) %>% 
  dplyr::summarise(
    n_events = n()
  )

manual_scoring_nm_summ <- manual_scoring_nm %>% 
  group_by(unmasked_video_file) %>% 
  dplyr::summarise(
    n_events = n()
  )

glimpse(auto_scoring_summ)
glimpse(manual_scoring_nm_summ)

# Merge this data together and make a plot
gg_res <- auto_scoring_summ %>% 
  dplyr::mutate(
    type = "automated_inference"
  ) %>% 
  bind_rows(
    manual_scoring_nm_summ %>%
      dplyr::rename(
        `video_file_name` = "unmasked_video_file"
      ) %>% 
      dplyr::mutate(
        type = "manual_scoring"
      )
  )

# This result makes sense given the coarser-grained temporal resolution of the automated scoring 
gg_res %>% 
  pivot_wider(id_cols = video_file_name, names_from = type, values_from = "n_events") %>%
  dplyr::arrange(automated_inference, manual_scoring) %>% 
  ggplot(aes(x = manual_scoring, y = automated_inference)) +
  geom_point() +
  geom_jitter(width = 0.25, height = 0.25) +
  geom_abline(slope = 1, linetype = "dotted") +
  scale_y_continuous(limits = c(0, max(gg_res$n_events)), breaks = seq(0, max(gg_res$n_events), 1)) +
  scale_x_continuous(limits = c(0, max(gg_res$n_events)), breaks = seq(0, max(gg_res$n_events), 1)) +
  xlab("Behavioral events obtained by manual scoring") +
  ylab("Behavioral events obtained by automated inference") +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank()
  )


```

Next steps:


- Consider asking validation questions to account for the coarser-grained temporal resolution of the automated scoring. For instance, when considering the sequence of behavioral events across the time unit represented by each video, how well do the automated inferences and the manual scoring agree on the behavioral label of the first event? Here we should ignore subject
```{r}

# Here we need to pull information from direction scored, inferredMovement_Location, and add in the perching events (see above) to match the majority of the manual scoring labels (aside from No Birds)
glimpse(auto_scoring_extraAssignedVids)

glimpse(manual_scoring_nm)
unique(manual_scoring_nm$Behavior)

# Focus on inside the container, entrances, and exits for now
# First, get the first behavioral label for each of the unique videos in the manually scored data
# The tricky thing here will be cases in which there are two behavioral states with the same start timestamp but for different subjects. In this case, we should focus on asking whether the automated score is at least contained within the first manual scores
tmp_manual <- manual_scoring_nm %>% 
  dplyr::select(Behavior, Start..s., Stop..s., unmasked_video_file) %>% 
  group_by(unmasked_video_file) %>% 
  arrange(Start..s.) %>% 
  # Filter to retain behavioral states per subject that started at the very start of each video
  dplyr::filter(
    Start..s. == 0
  ) %>% 
  ungroup()

# Then for each of these rows in the manually scored data, get the first automated score for the corresponding video

# i <- 1
manual_auto_integr <- data.table::rbindlist(lapply(1:nrow(tmp_manual), function(i){
  
  tmp <- auto_scoring_extraAssignedVids %>% 
    dplyr::filter(
      grepl(tmp_manual$unmasked_video_file[i], assigned_recording_event)
    ) %>%
    arrange(start) %>% 
    slice(1) %>% 
    dplyr::select(direction_scored, inferredMovement_Location)
  
  # The inferred movement location column holds the score "inside container" or INSC in the manually scored data when an exit or entrance wasn't detected
  if(is.na(tmp$direction_scored)){
    
    inference <- tmp$inferredMovement_Location
  
  } else {
    
    inference <- tmp$direction_scored
  }
  
  return(
    
    tmp_manual %>% 
      slice(i) %>% 
      dplyr::mutate(
        first_auto_inference = inference
      )
  )
  
}))

# TKTK continue
View(manual_auto_integr)


```

Work on the following updates:

- Start validation analyses at a very coarse-grained scale with the manual scoring as a baseline:

 0. Automated data processing: Iterate over parameters that we use for the automated scoring, especially the temporal thresholds used for clustering, to create different automated scoring datasets for the analyses below

  1. First, how well does the integration of 3 movement sensors capture whether birds are inside of the container or not? Can we generally use the automated scores to infer that sensor activity was due to the birds or not? For this analysis, perform integration of data from the 3 sensors for all days of data collection, and then index the scored activities using the timestamps of the manually scored video recording events with a time buffer around them. Focus on asking whether the automated and manual scores capture activity by a bird at the container entrance or inside of the container, and how often the automated scoring is capturing activity that cannot be attributed to a bird. Is it possible to use periods of time with no sensor activity to infer that no bird was present inside of the container?
  
  2. Second, ask how well the integrated dataset of movement sensors agrees with the first behavioral score attributed to each video recording event by manual scoring. This is a finer-grained validation to ask if behavioral categories associated with the video recording events match between the two methods. Check out my notes above that detail expectations - there will be more scores per video recording event using the manual method
  
  3. Third, use the videos selected for entire days of data collection for another fine-grained validation analysis to ask how well the automated integration of sensors picks up long sequences of behavioral events throughout the day, compared to the manually scored videos. Here the caveat will be that we do not have continuous video recordings, so there will likely be more activity scored across each day by the automated methods. Instead, focus on asking how well the two methods concide in identifying sequential behaviors across the ethogram used for validation (perching, enter, exit, inside container)
  
  4. Ask how reliably we captured individual identity with RFID compared to the manually scored videos, and how often no PIT tags or only a single PIT was detected when manual video scoring detected 1 tagged adult or 2 inside of the nest container


Once the pipeline above is developed, then run the full validation analysis pipeline with all 4045 video recording events
