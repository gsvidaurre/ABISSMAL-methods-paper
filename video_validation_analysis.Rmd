---
title: "Interobserver reliability"
author: "Grace Smith-Vidaurre"
date: "2024-07-18"
output: html_document
---

Purpose: Perform a validation analysis of automatically scored behaviors (e.g. behavioral inferences) obtained by ABISSMAL. The computational analyses available in ABISSMAL facilitate integrating data across movement sensors to derive coarse-grained behavioral inferences. Here, we will validate those behavioral inferences for a subsample of the full set of videos used in the methods manuscript, using a baseline dataset obtained by manual video scoring to obtain coarse-grained behavioral states for adult birds and juveniles. The current plan for this validation analysis includes 1) assessing whether the automated and manual scoring return similar levels of overall activity per unique recording event, 2) whether the automated and manual scoring return similar levels of overall activity per behavioral state in each unique recording event, 3) the degree to which the timestamps for these automated and manual scores agree with one another (this will be a bit tricky given that some of the movement sensors will detect movement before the videos), 4) we can also iterate over parameter combinations for the automated scoring to ask how these decisions influence ageement with the manual dataset.

The manually scored data needs to be concatenated and checked prior to running the final validation analysis. Also, Summer noticed some video recording events had videos switched in order, so we have to pre-process the manually scored dataset to fix these issues for the videos she caught that had this issue.

Finally, the validation analysis will be planned out and run with a random subsample of the videos used for this analysis.

```{r}

knitr::opts_knit$set(eval = TRUE, echo = TRUE)

```

```{r package and paths, warning = FALSE, message = FALSE}

# Clean the global environment
rm(list = ls())

X <- c("tidyverse", "pbapply", "data.table")

invisible(lapply(X, library, character.only = TRUE))

# Path to the aggregated BORIS spreadsheet in wide format across all video recording events
path <- "~/Desktop/ABISSMAL_validation_data"

# Path to the combined raw movement sensor data
sensor_path <- file.path(path, "raw_combined")

seed <- 888

```

Read in the aggregated manual scoring spreadsheet in which each row is a unique behavioral state per subject within a recording event.
```{r}

manual_scoring <- read.csv(file.path(path, "BORIS_ManualScoring_Aggregated_pre-processed.csv"))
glimpse(manual_scoring)

# View(manual_scoring)

```

Next steps:

1. Data pre-processing of the switched recording events that Summer identified. Look at the spreadsheet called "BORIS_VideoList_TimeFlipped.csv". We could i) exclude these videos from analyses, or ii) we could try to flip the manual scores given the timestamps of the scores and the videos themselves, or iii) if one video has extremely short duration, then drop the scores for that video in the pair. TKTK keep thinking about a solution 

2. We need to choose a subsample of these videos to develop the validation analysis pipeline.

```{r}

# Randomly select 50 video recording events from the current version of the aggregated manual scoring results in wide format

# First get all of the unique BORIS Observation IDs that correspond to unique recording events
unique_obs_ids <- unique(manual_scoring$Observation.id)
head(unique_obs_ids)

# There should be 4045 Observation IDs to match the unique recording events that Summer scored, looks good
length(unique_obs_ids)

set.seed(seed)
subsample <- sample(unique_obs_ids, 50, replace = FALSE)

subsample

```

3. Start writing out the steps of the validation analysis, run these with the subsample of videos chosen above

Pseudocode for the analysis:

i) Data Processing:

  - Get the manual scoring data from BORIS in aggregated format (done)
  
  - Decide how to handle the time-flipped videos (in progress)

  - Get the raw data across movement sensors used for analyses in the current methods manuscript
  
```{r}

# The movement sensor data that we are using here is the raw data that we have used in the methods manuscript, and it's already been combined across days of data collection using ABISSMAL functions. We have one spreadsheet per sensor type, and the beam breaker spreadsheet contains data for 2 pairs of beam breakers
list.files(sensor_path)

# We will use this data to index the raw movement sensor data by the timestamps of the videos that Summer scored. The spreadsheet for the video recording events should have the timestamps or at least the start time for each unique recording event. The timestamp column in this spreadsheet should correspond to the time at which the video camera detected motion and began recording the post-movement video. TKTK need to check the Video module code for ABISSMAL to verify. Onve this is verified, then we can decide how to proceed with timestamps in this spreadsheet plus the known durations of each video in each unique recording event (may need to do more data wrangling to figure out durations). A quick check of the spreadsheet below indicates we should use the column of Media File Duration to extract the exact duration of each video in each unique recording event (lots of variability in the duration of the pre-movement video recorded using the ring buffer)
video_events <- read.csv(file.path(sensor_path, "combined_raw_data_Video.csv"))
glimpse(video_events)

View(video_events)

```

  
  - Index the raw movement sensor data by the start and end timestamps of all of the videos used for manual scoring for validation
  
```{r}



```
  
  - Perform automated behavioral scoring of the indexed raw movement sensor data, and iterate over a range of temporal thresholds used for clustering detections across sensors (for pre-processing, use the same parameters as in the methods manuscript)

  - Check that the automated and manual datasets are in similar format

ii) Analyzing overall patterns of activity:

  - Do both scoring methods return similar levels of activity for each unique recording event?
  - Plot the levels of activity for each recording event by each method against each other, and repeat this process for different temporal thresholds used for the automated scoring

iii) Analyzing overall patterns of activity by behavioral state:

  - Do both scoring methods return similar levels of activity for behavioral state across each unique recording event?

iv) How do the timestamps of scored behavioral states compare between methods?

*** As the best methods comparison: how badly are our statistical/biological inferences skewed using one method or the other? Iterate over parameters that we use for the automated scoring, especially the temporal thresholds used for clustering


5. Run the full validation analysis pipeline with all 4045 video recording events
